{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370a5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55354f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(pruned_model):\n",
    "    mask = []\n",
    "    for i in range(1,len(pruned_model.layers)):\n",
    "        weights = np.array((pruned_model.layers[i].get_weights()[0] != 0)*1.0)\n",
    "        biases = np.array((pruned_model.layers[i].get_weights()[1] != 0)*1.0)\n",
    "        layer = [weights, biases]\n",
    "        mask.append(layer)\n",
    "    return mask\n",
    "\n",
    "def set_model(init_model,pruned_model):\n",
    "    mask = get_mask(pruned_model)\n",
    "    for i in range(1,len(init_model.layers)):\n",
    "        layer = []\n",
    "        weights = init_model.layers[i].get_weights()[0]\n",
    "        biases = init_model.layers[i].get_weights()[1]\n",
    "        pruned_model.layers[i].set_weights([np.where(mask[i-1][0] == 0, 0, weights), biases])# pruning doesn't zero out biases\n",
    "    return pruned_model                                                                      # so we just copy the init-biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00e6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all subsequent MINE functions work as above, just need different distributions and their dimensions\n",
    "\n",
    "def get_mine_x_d1(model):\n",
    "    model_1 = keras.models.clone_model(model) # only 1 model copy needed cause we only predict one distrib.\n",
    "    \n",
    "    distrib_x = x_train # first distribution is the input\n",
    "    \n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    outputs = model_1.layers[1](inputs)\n",
    "    distrib_y_model = keras.Model(inputs=inputs, outputs=outputs) # second distribution here is outputs Dense_1\n",
    "    \n",
    "    distrib_y_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_y = distrib_y_model.predict_on_batch(x_train) # d1_distribution are predicted outputs of Dense_1 given x_train\n",
    "    \n",
    "    mine = MINE(x_dim=input_dim, y_dim=d1_dim) # statistics network construction\n",
    "    fit_loss_history, mutual_info = mine.fit(distrib_x, distrib_y, epochs=10, batch_size=128) # MI estimation\n",
    "    \n",
    "    return fit_loss_history, mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921473a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mine_x_d2(model):\n",
    "    \n",
    "    model_1 = keras.models.clone_model(model) # distrib. 1 is Input, hence only need to predict the second\n",
    "    \n",
    "    distrib_x = x_train # first distribution is the input\n",
    "    \n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = model_1.layers[1](inputs)\n",
    "    outputs = model_1.layers[2](x)\n",
    "    distrib_y_model = keras.Model(inputs=inputs, outputs=outputs) # second distribution here is outputs of Dense_2\n",
    "    \n",
    "    distrib_y_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_y = distrib_y_model.predict_on_batch(x_train) # d2_distribution are predicted outputs of Dense_2 given x_train\n",
    "    \n",
    "    mine = MINE(x_dim=input_dim, y_dim=d2_dim) # statistics network construction\n",
    "    fit_loss_history, mutual_info = mine.fit(distrib_x, distrib_y, epochs=10, batch_size=128) # MI estimation\n",
    "    \n",
    "    return fit_loss_history, mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c66bb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mine_x_o(model):\n",
    "    model_1 = keras.models.clone_model(model) # will predict second distrib. one copy of full model for outputs predictions\n",
    "    \n",
    "    distrib_x = x_train # first distribution is the input\n",
    "    \n",
    "    distrib_y_model = model_1 # second distribution here is model outputs\n",
    "    \n",
    "    distrib_y_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_y = distrib_y_model.predict_on_batch(x_train) # predicted model outputs given x_train\n",
    "    \n",
    "    mine = MINE(x_dim=input_dim, y_dim=output_dim) # statistics network construction\n",
    "    fit_loss_history, mutual_info = mine.fit(distrib_x, distrib_y, epochs=10, batch_size=128) # MI estimation\n",
    "    \n",
    "    return fit_loss_history, mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb5dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mine_d1_d2(model):\n",
    "    model_1 = keras.models.clone_model(model) # copies of model for distibution predictions on x_input\n",
    "    model_2 = keras.models.clone_model(model)\n",
    "    \n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\") \n",
    "    outputs = model_1.layers[1](inputs) # first distribution are outputs of first hidden layer\n",
    "    distrib_x_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    distrib_x_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_x = distrib_x_model.predict_on_batch(x_train) # use predict function to create layer output\n",
    "    \n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\") # second distribution, outputs of second hidden layer, same as above\n",
    "    x = model_2.layers[1](inputs)\n",
    "    outputs = model_2.layers[2](x)\n",
    "    distrib_y_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    distrib_y_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_y = distrib_y_model.predict_on_batch(x_train) # d2_layer outputs\n",
    "    \n",
    "    mine = MINE(x_dim=d1_dim, y_dim=d2_dim) # construction of statistics network for MI estimation, giving distibution dimensions\n",
    "    fit_loss_history, mutual_info = mine.fit(distrib_x, distrib_y, epochs=10, batch_size=128) # estimate MI on distibution samples\n",
    "    \n",
    "    return fit_loss_history, mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7637a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mine_d1_o(model):\n",
    "    model_1 = keras.models.clone_model(model) # copies of model for distibution predictions on x_input\n",
    "    model_2 = keras.models.clone_model(model)\n",
    "    \n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\") \n",
    "    outputs = model_1.layers[1](inputs) # first distribution are outputs of first hidden layer\n",
    "    distrib_x_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    distrib_x_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_x = distrib_x_model.predict_on_batch(x_train) # use predict function to create layer output\n",
    "    \n",
    "    distrib_y_model = model_2 # distrib. 2 are the model outputs\n",
    "    \n",
    "    distrib_y_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_y = distrib_y_model.predict_on_batch(x_train) # d2_layer outputs\n",
    "    \n",
    "    mine = MINE(x_dim=d1_dim, y_dim=output_dim) # construction of statistics network for MI estimation, giving distibution dimensions\n",
    "    fit_loss_history, mutual_info = mine.fit(distrib_x, distrib_y, epochs=10, batch_size=128) # estimate MI on distibution samples\n",
    "    \n",
    "    return fit_loss_history, mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecbdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mine_d2_o(model):\n",
    "    model_1 = keras.models.clone_model(model) # copies of model for distibution predictions on x_input\n",
    "    model_2 = keras.models.clone_model(model)\n",
    "    \n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\") \n",
    "    x = model_1.layers[1](inputs)\n",
    "    outputs = model_1.layers[2](x) # first distribution are outputs of second hidden layer\n",
    "    distrib_x_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    distrib_x_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_x = distrib_x_model.predict_on_batch(x_train) # use predict function to create layer output\n",
    "    \n",
    "    distrib_y_model = model_2 # distrib. 2 are the model outputs\n",
    "    \n",
    "    distrib_y_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    distrib_y = distrib_y_model.predict_on_batch(x_train) # d2_layer outputs\n",
    "    \n",
    "    mine = MINE(x_dim=d2_dim, y_dim=output_dim) # construction of statistics network for MI estimation, giving distibution dimensions\n",
    "    fit_loss_history, mutual_info = mine.fit(distrib_x, distrib_y, epochs=10, batch_size=128) # estimate MI on distibution samples\n",
    "    \n",
    "    return fit_loss_history, mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a1862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a quick function to iteratively determine sparsity level given a certain number of pruning operations with a given rate\n",
    "def calc_sparsity(iteration, pruning_rate):\n",
    "    sparsity = 100 * (1 - pruning_rate) ** (iteration+1)\n",
    "    return 100-sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa29d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(list_of_lists):\n",
    "    average = np.average(list_of_lists, axis=0)\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4a0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of loss_scores and averages out the dimension of the number of experiments -> 10 runs > average loss of 10 runs\n",
    "def calculate_average_loss(losses_list):\n",
    "    avg_losses = np.array(losses_list)\n",
    "    avg_losses = np.average(avg_losses, axis=0)\n",
    "\n",
    "    return avg_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
