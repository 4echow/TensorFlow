{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30da1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "#from tensorflow.keras.callbacks import Callback\n",
    "#%load_ext tensorboard\n",
    "%matplotlib widget\n",
    "import lottery_ticket_pruner\n",
    "from lottery_ticket_pruner import LotteryTicketPruner, PrunerCallback\n",
    "from mine import MINE\n",
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737f28d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28) (60000,)\n",
      "Test data shape: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset using TensorFlow\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Display the shapes of the training and test datasets\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# reshape data as 2D numpy arrays\n",
    "# convert to float32 and normalize grayscale for better num. representation\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# The tutorial reserved 10.000 training samples for validation, we change to 5.000 \n",
    "# as that is what Frankle and Carbin did in their paper\n",
    "x_val = x_train[-5000:]\n",
    "y_val = y_train[-5000:]\n",
    "x_train = x_train[:-5000]\n",
    "y_train = y_train[:-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8951c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "batch_size = 60 # batchsize, 60 images per weight update\n",
    "epochs = 10 # nr. of epochs we train our models\n",
    "validation_split = 1/11 # 5000 val 55000 train data\n",
    "input_dim = 784 # input_distribution size for MINE\n",
    "d1_dim = 100 # first hidden layer distribution size for MINE\n",
    "d2_dim = 30  # second hidden layer distribution size for MINE\n",
    "output_dim = 10 # output_distribution dim for MINE\n",
    "pruning_rate = 0.5 # pruning rate for LTH iterative Pruning -> removes pruning_rate% of lowest magnitude weights in an iteration\n",
    "pruning_iterations = 5 # number of iterations for applying the pruning rate iteratively -> 1 time : 20% sparse, 13 times : ~95% sparse\n",
    "averaging_iterations = 2 # number of total experimental runs to average for graph representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bc3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " digits (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                3030      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                310       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,840\n",
      "Trainable params: 81,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() # clearing backend right at start, just in case\n",
    "\n",
    "inputs = keras.Input(shape=(input_dim,), name=\"digits\") # Functional build of a 2-hidden layer fully connected MLP\n",
    "x = layers.Dense(d1_dim, activation=\"relu\", name=\"dense_1\")(inputs) # methods made no mention of the activaton function specifically\n",
    "x = layers.Dense(d2_dim, activation=\"relu\", name=\"dense_2\")(x) # ReLU is standard, as all available implementations seem to use it too\n",
    "outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"predictions\")(x)  # softmax activation for multi-class classification\n",
    "\n",
    "base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411b5c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved initialization\n",
    "base_model.load_weights(\"init_weights.h5\")\n",
    "init_weights = base_model.get_weights() # init weights for Lotter Ticket reset to initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ee598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "------------------------\n",
      "------------------------\n",
      "Experimental run number: 1\n",
      "------------------------\n",
      "------------------------\n",
      "------------------------\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 749us/step - loss: 2.3140 - sparse_categorical_accuracy: 0.0917\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.5569e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.0039e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.3256e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -4.5562e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -4.7707e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.1962e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -5.8073e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -5.5608e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -7.1638e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -6.7619e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.1464e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -2.2585e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -2.7420e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -2.9940e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -3.2931e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -3.4168e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -3.4932e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -3.5950e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -3.7537e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -3.7828e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 6s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.4490e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.5830e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.0449e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -8.2399e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -9.3487e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -9.8921e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.0594e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -1.1243e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.1661e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.2459e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 7s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3515e+00 - 693ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.2488e+00 - 401ms/epoch - 933us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4149e+00 - 401ms/epoch - 933us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6774e+00 - 399ms/epoch - 928us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.8186e+00 - 399ms/epoch - 928us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.8986e+00 - 402ms/epoch - 935us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.9255e+00 - 403ms/epoch - 937us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -3.0849e+00 - 400ms/epoch - 930us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.0832e+00 - 401ms/epoch - 933us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.0798e+00 - 408ms/epoch - 949us/step\n",
      "1719/1719 [==============================] - 1s 728us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.4375e-01 - 693ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.7902e-01 - 399ms/epoch - 928us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.4726e-01 - 401ms/epoch - 933us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -6.9085e-01 - 398ms/epoch - 926us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -7.4277e-01 - 399ms/epoch - 928us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -8.2174e-01 - 398ms/epoch - 926us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -8.9195e-01 - 396ms/epoch - 921us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -9.5780e-01 - 400ms/epoch - 930us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0098e+00 - 399ms/epoch - 928us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.0500e+00 - 401ms/epoch - 932us/step\n",
      "1719/1719 [==============================] - 1s 718us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1114e-01 - 873ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.5878e-01 - 417ms/epoch - 970us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.3433e-01 - 421ms/epoch - 979us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -3.8440e-01 - 418ms/epoch - 972us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.1333e-01 - 418ms/epoch - 972us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.3579e-01 - 419ms/epoch - 974us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -4.5769e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -4.8115e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.1516e-01 - 421ms/epoch - 979us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.4505e-01 - 419ms/epoch - 974us/step\n",
      "1719/1719 [==============================] - 1s 731us/step\n",
      "estimated mutual information x_d1: 9.42943\n",
      "estimated mutual information x_d2: 3.5818279\n",
      "estimated mutual information x_o: 1.3430927\n",
      "estimated mutual information d1_d2: 2.767986\n",
      "estimated mutual information d1_o: 1.0002216\n",
      "estimated mutual information d2_o: 0.49525225\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "fully connected model, pre-train: loss: 2.3140008449554443 acc: 0.0917000025510788\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, pre-train, first 10 arr: \n",
      "[[ 0.10372657 -0.23062739  0.17682165 -0.08873466  0.23557895 -0.03147158\n",
      "  -0.23462272  0.2651338  -0.12489226 -0.03261241]\n",
      " [-0.09772784  0.3317693  -0.33232167 -0.00817764 -0.12354863  0.06477183\n",
      "  -0.37804726 -0.2229637   0.386456   -0.13857591]\n",
      " [-0.3368967  -0.1321938  -0.1171802  -0.16798306  0.21808767  0.15431464\n",
      "  -0.21958955 -0.27966002 -0.30258098 -0.09193829]\n",
      " [-0.3046082  -0.33831993  0.37752926  0.2601511  -0.04892328 -0.06275451\n",
      "  -0.0382643   0.30578887  0.26934433  0.17941535]\n",
      " [ 0.15212536  0.24618846 -0.18440057 -0.27350062 -0.24141666 -0.20426965\n",
      "  -0.09346464 -0.20180817 -0.24661073 -0.22613518]\n",
      " [-0.03369612 -0.18294892 -0.28426045  0.2934637  -0.1341235   0.3308233\n",
      "   0.11271024 -0.15089689  0.08726564 -0.33876473]\n",
      " [-0.1912756  -0.08345768  0.1534971   0.38107044  0.15828174 -0.16249561\n",
      "   0.29039502 -0.32429057  0.2932816  -0.04296836]\n",
      " [ 0.18538827  0.07226792  0.00952986 -0.21432622  0.10683995  0.05759257\n",
      "   0.30292875  0.3747828  -0.3317164   0.09456533]\n",
      " [-0.09649152 -0.2496497   0.2983873  -0.18555251  0.12270099  0.24197662\n",
      "  -0.31861964 -0.31165904 -0.37963566 -0.07235166]\n",
      " [ 0.3217913  -0.02085996 -0.16163169 -0.12485653 -0.02415416  0.1396634\n",
      "  -0.37486386  0.06521994  0.26913399  0.13854533]]\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 830us/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9757\n",
      "fully connected model, trained: loss: 0.0904313251376152 acc: 0.9757000207901001\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "early append of _init done\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.6759e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -3.9684e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -4.4080e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -4.6974e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.6853e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -4.6546e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.4596e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -5.6075e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -6.0973e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -7.5909e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.3691e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -2.5635e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.9488e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -3.1427e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.2482e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -3.4908e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.5064e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -3.6325e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.7066e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.8428e+00 - 1s/epoch - 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3442e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.5679e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.5774e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -9.2585e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -1.0911e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -1.2155e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -1.2802e+00 - 941ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -1.3665e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -1.4419e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.5016e+00 - 1s/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2889e+00 - 904ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.1092e+00 - 445ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4058e+00 - 445ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.5336e+00 - 445ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.7274e+00 - 445ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7714e+00 - 448ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.8229e+00 - 448ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.9244e+00 - 446ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9766e+00 - 446ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.0212e+00 - 446ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 746us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.3072e-01 - 724ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.3116e-01 - 425ms/epoch - 989us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.5834e-01 - 425ms/epoch - 989us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -8.0379e-01 - 428ms/epoch - 996us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -9.2242e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.0016e+00 - 426ms/epoch - 991us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.0684e+00 - 426ms/epoch - 991us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.1035e+00 - 426ms/epoch - 991us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.1550e+00 - 425ms/epoch - 989us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.1623e+00 - 426ms/epoch - 991us/step\n",
      "1719/1719 [==============================] - 1s 736us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.2693e-01 - 684ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.1330e-01 - 395ms/epoch - 919us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -5.9207e-01 - 394ms/epoch - 916us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -6.5728e-01 - 394ms/epoch - 917us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -6.8367e-01 - 396ms/epoch - 921us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -7.1202e-01 - 398ms/epoch - 926us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -7.3368e-01 - 395ms/epoch - 919us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -7.5312e-01 - 394ms/epoch - 917us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -7.6968e-01 - 397ms/epoch - 924us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -7.9437e-01 - 393ms/epoch - 914us/step\n",
      "1719/1719 [==============================] - 1s 706us/step\n",
      "estimated mutual information x_d1: 8.455219\n",
      "estimated mutual information x_d2: 3.8109882\n",
      "estimated mutual information x_o: 1.5615295\n",
      "estimated mutual information d1_d2: 2.6559606\n",
      "estimated mutual information d1_o: 1.0895363\n",
      "estimated mutual information d2_o: 0.74293554\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, trained, fully connected model, first 10 arr: \n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "prune_mask calculated\n",
      "\n",
      "Iteration 1: making 50.00% sparse large_final\n",
      "Iteration 1: making 50.00% sparse random\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 898us/step - loss: 0.0767 - sparse_categorical_accuracy: 0.9762\n",
      "50.00% sparse large_final: loss: 0.07667475193738937 acc: 0.9761999845504761\n",
      "313/313 [==============================] - 0s 834us/step - loss: 2.3685 - sparse_categorical_accuracy: 0.0992\n",
      "50.00% sparse large_final, init_weights: loss: 2.3685452938079834 acc: 0.09920000284910202\n",
      "313/313 [==============================] - 0s 814us/step - loss: 0.0915 - sparse_categorical_accuracy: 0.9734\n",
      "50.00% sparse random: loss: 0.09148237854242325 acc: 0.9733999967575073\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.6447e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -3.9457e+00 - 985ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.3259e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.7298e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -4.9796e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -5.2425e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.9099e+00 - 811ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -5.8622e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -6.6605e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -6.9366e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.7119e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -2.9354e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -3.2304e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.3313e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.5629e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.6547e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.6851e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.7688e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.9368e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -4.0796e+00 - 879ms/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.8205e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -6.3650e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -1.0096e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -1.2104e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -1.3651e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 4s - loss: -1.4105e+00 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -1.4398e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -1.5283e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -1.5792e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -1.6108e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3168e+00 - 747ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.1489e+00 - 446ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4793e+00 - 427ms/epoch - 993us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6304e+00 - 428ms/epoch - 996us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.7266e+00 - 447ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7445e+00 - 449ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.8983e+00 - 448ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.9028e+00 - 440ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9963e+00 - 411ms/epoch - 956us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.0115e+00 - 401ms/epoch - 933us/step\n",
      "1719/1719 [==============================] - 1s 743us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.6844e-01 - 970ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -7.5415e-01 - 501ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -9.6316e-01 - 484ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -1.0516e+00 - 501ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -1.1061e+00 - 455ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.1638e+00 - 479ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.2125e+00 - 471ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.2605e+00 - 479ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.3104e+00 - 474ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.3712e+00 - 476ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 794us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.5069e-01 - 723ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.7218e-01 - 447ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -4.6838e-01 - 435ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.9935e-01 - 437ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -5.2962e-01 - 446ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.4076e-01 - 425ms/epoch - 989us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.5160e-01 - 456ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.8926e-01 - 427ms/epoch - 993us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -6.2077e-01 - 458ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.2555e-01 - 455ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 802us/step\n",
      "Epoch 1/10\n",
      "430/430 - 4s - loss: -2.7612e+00 - 4s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 4s - loss: -4.1926e+00 - 4s/epoch - 9ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 4s - loss: -4.5343e+00 - 4s/epoch - 9ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -4.8567e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -5.1334e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.3104e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -5.5067e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -5.1846e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 4s - loss: -5.9467e+00 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -6.1470e+00 - 3s/epoch - 7ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3901e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.5201e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.9231e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.1670e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -3.3157e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.3710e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.4581e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.5989e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.6114e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.7306e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 7s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.1921e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -4.5242e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -7.8085e-01 - 3s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -9.7121e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.1180e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.2167e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -1.3333e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.4357e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.4914e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.5870e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 7s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1528e+00 - 943ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -1.9236e+00 - 449ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.0627e+00 - 451ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.2832e+00 - 449ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.4447e+00 - 447ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.5094e+00 - 449ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.6276e+00 - 447ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.6680e+00 - 450ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.7931e+00 - 450ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.8382e+00 - 449ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 773us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.4482e-01 - 734ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -6.3030e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -7.7082e-01 - 448ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -8.9821e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -1.0269e+00 - 446ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.0941e+00 - 443ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.1684e+00 - 444ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.2033e+00 - 443ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.2379e+00 - 444ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.2697e+00 - 444ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 770us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3648e-01 - 715ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.5694e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.9964e-01 - 425ms/epoch - 989us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -3.5189e-01 - 425ms/epoch - 988us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -3.9127e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.2267e-01 - 433ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -4.4532e-01 - 430ms/epoch - 999us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -4.7223e-01 - 433ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.0155e-01 - 433ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.2467e-01 - 439ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 767us/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -2.7406e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -4.0352e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.5475e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.7560e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -5.2901e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.3795e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.4854e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -5.6259e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.4131e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -8.0476e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.2320e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -2.4021e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.8614e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.0803e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -3.2817e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.3792e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.5192e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.4656e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.6419e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -3.6725e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 8s 5ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.0306e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -4.3843e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.3103e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -9.1749e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -1.0663e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1692e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.2829e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.3999e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -1.4801e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.5331e+00 - 2s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2048e+00 - 699ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -1.9793e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.2726e+00 - 407ms/epoch - 947us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.4247e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.5558e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.6762e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.7527e+00 - 407ms/epoch - 947us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.7450e+00 - 409ms/epoch - 951us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.7939e+00 - 444ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.8314e+00 - 428ms/epoch - 996us/step\n",
      "1719/1719 [==============================] - 1s 785us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.9662e-01 - 748ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.9463e-01 - 433ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.4742e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.6914e-01 - 430ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.7525e-01 - 443ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.0197e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.1137e+00 - 405ms/epoch - 942us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.1839e+00 - 418ms/epoch - 972us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.2458e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.3078e+00 - 429ms/epoch - 998us/step\n",
      "1719/1719 [==============================] - 2s 813us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2601e-01 - 713ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.6168e-01 - 408ms/epoch - 949us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.3979e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.1542e-01 - 428ms/epoch - 996us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.9233e-01 - 417ms/epoch - 970us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.2090e-01 - 413ms/epoch - 961us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.6027e-01 - 435ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.7621e-01 - 421ms/epoch - 979us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.8884e-01 - 446ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.1660e-01 - 427ms/epoch - 993us/step\n",
      "1719/1719 [==============================] - 1s 788us/step\n",
      "estimated mutual information x_d1, large_final: 8.091858\n",
      "estimated mutual information x_d2, large_final: 3.609764\n",
      "estimated mutual information x_o, large_final: 1.4362061\n",
      "estimated mutual information d1_d2, large_final: 2.6645029\n",
      "estimated mutual information d1_o, large_final: 1.3055701\n",
      "estimated mutual information d2_o, large_final: 0.59759194\n",
      "estimated mutual information x_d1, large_final, init_weights: 7.9331303\n",
      "estimated mutual information x_d2, large_final, init_weights: 3.6919186\n",
      "estimated mutual information x_o, large_final, init_weights: 1.6099212\n",
      "estimated mutual information d1_d2, large_final, init_weights: 2.5994718\n",
      "estimated mutual information d1_o, large_final, init_weights: 1.227075\n",
      "estimated mutual information d2_o, large_final, init_weights: 0.4779939\n",
      "estimated mutual information x_d1, random: 10.115383\n",
      "estimated mutual information x_d2, random: 3.3502269\n",
      "estimated mutual information x_o, random: 1.6111978\n",
      "estimated mutual information d1_d2, random: 2.4980056\n",
      "estimated mutual information d1_o, random: 1.1354783\n",
      "estimated mutual information d2_o, random: 0.58939767\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, 50.00% sparse large_final, first 10 arr: \n",
      "[[-0.08089512 -0.5320751  -0.         -0.19582917  0.46217754 -0.13981594\n",
      "  -0.74415654  0.28989303  0.          0.14392638]\n",
      " [-0.          0.591638   -0.76981866 -0.         -0.05020117  0.04049125\n",
      "  -0.5367696  -0.41422912 -0.          0.09658336]\n",
      " [-0.71094745 -0.14929925  0.         -0.2090191  -0.          0.24701937\n",
      "   0.         -0.6941212  -0.26052022  0.        ]\n",
      " [-0.         -0.         -0.          0.35708582 -0.         -0.2013065\n",
      "  -0.62162626  0.24603324  0.53296083  0.3105871 ]\n",
      " [ 0.22790612  0.          0.          0.         -0.8653873  -0.\n",
      "  -0.10452288 -0.         -0.8552458  -0.57849216]\n",
      " [ 0.         -0.71869785  0.         -0.         -0.          0.41000825\n",
      "   0.16313484 -0.76591146  0.          0.        ]\n",
      " [-0.26157343  0.06099439 -0.         -0.          0.2745021  -0.55001605\n",
      "   0.43742898 -0.          0.          0.        ]\n",
      " [ 0.15177315  0.          0.45243782 -0.         -0.25241852  0.\n",
      "  -0.          0.73696834 -0.699427   -0.        ]\n",
      " [-0.00735557  0.          0.62307656 -0.25078863 -0.         -0.\n",
      "   0.         -0.         -0.         -0.        ]\n",
      " [ 0.38937637 -0.6145599  -0.3298399  -0.3972948  -0.00485919  0.21040423\n",
      "   0.         -0.         -0.          0.3172908 ]]\n",
      "output layer weight mask, 50.00% sparse large_final, init_weights, first 10 arr: \n",
      "[[ 0.16793531 -0.19815828  0.00577998  0.35408348  0.1492154   0.12632018\n",
      "   0.24829686  0.29094827 -0.28615838  0.26713395]\n",
      " [ 0.38530135  0.17435563  0.12733388 -0.372132    0.22066021  0.06476378\n",
      "   0.1588955   0.05780235  0.30895227  0.3318572 ]\n",
      " [-0.17773443 -0.24720004  0.21787155 -0.11562759 -0.17134291  0.3837418\n",
      "  -0.22211751 -0.10500547  0.05907276  0.37995923]\n",
      " [ 0.31963456 -0.27910802 -0.3465212   0.32741535 -0.2596431  -0.18591106\n",
      "   0.22975498  0.13370961 -0.09820801 -0.21896552]\n",
      " [ 0.29762203  0.12608945  0.38343257 -0.2541073  -0.10766399 -0.11768603\n",
      "  -0.220159    0.17379111 -0.31485507 -0.19066072]\n",
      " [ 0.04082459  0.10627577 -0.33504096  0.08793187  0.30454677  0.22761899\n",
      "   0.172822    0.16477382 -0.2913881   0.11035162]\n",
      " [ 0.26548386  0.01757878 -0.36683932 -0.22290719 -0.1156061  -0.16576977\n",
      "   0.01223603 -0.03303978 -0.37626788 -0.3815471 ]\n",
      " [ 0.37485212  0.22040045  0.38529146 -0.12884974 -0.32977283 -0.16270763\n",
      "  -0.14514084 -0.11418906  0.2590742   0.38547862]\n",
      " [ 0.01368871  0.05117202  0.36923832  0.22120643  0.15816945 -0.2112902\n",
      "   0.3741868   0.21973127 -0.21325877  0.0313988 ]\n",
      " [-0.06928307  0.12325382  0.26712888  0.17257756 -0.1154494   0.1961788\n",
      "  -0.32805893  0.31263125  0.3788075  -0.05266044]]\n",
      "output layer weight mask, 50.00% sparse random, first 10 arr: \n",
      "[[-0.         -0.5625156   0.         -0.05867655 -0.          0.\n",
      "  -0.45245063 -0.         -0.31800184  0.        ]\n",
      " [-0.         -0.          0.          0.         -0.83786726  0.24082775\n",
      "  -0.47783306 -0.         -0.         -0.33416882]\n",
      " [-0.          0.         -0.6070152  -0.5630623   0.7134129   0.00110581\n",
      "  -0.5770775  -0.         -0.85800683 -0.04071307]\n",
      " [-0.         -0.          0.63242334  0.34721726 -0.3185079  -0.24691509\n",
      "  -0.         -0.          0.38453773  0.3202867 ]\n",
      " [-0.5726253   0.58054984  0.         -1.1001797  -0.         -0.\n",
      "   0.11945278 -0.08138833 -0.2593247  -0.35551623]\n",
      " [-0.         -0.09276917 -0.20200941  0.2620229   0.          0.23973939\n",
      "   0.         -0.22308226  0.08980227 -0.        ]\n",
      " [-0.6384093  -0.18520151 -0.07528591  0.45114437  0.37453446  0.\n",
      "  -0.          0.          0.6035871  -0.04173436]\n",
      " [-0.         -0.4149132  -0.          0.          0.         -0.\n",
      "   0.          0.87196124 -0.         -0.        ]\n",
      " [ 0.         -0.56572026 -0.         -0.676956   -0.          0.\n",
      "   0.         -0.59765226  0.         -0.08361532]\n",
      " [-0.         -0.         -0.          0.          0.07047471  0.12288005\n",
      "  -0.6961487   0.07065426  0.4744689   0.37457553]]\n",
      "prune_mask calculated\n",
      "\n",
      "Iteration 2: making 75.00% sparse large_final\n",
      "Iteration 2: making 75.00% sparse random\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 785us/step - loss: 0.0894 - sparse_categorical_accuracy: 0.9723\n",
      "75.00% sparse large_final: loss: 0.08941306918859482 acc: 0.9722999930381775\n",
      "313/313 [==============================] - 0s 883us/step - loss: 2.3685 - sparse_categorical_accuracy: 0.0992\n",
      "75.00% sparse large_final, init_weights: loss: 2.3685452938079834 acc: 0.09920000284910202\n",
      "313/313 [==============================] - 0s 801us/step - loss: 0.1308 - sparse_categorical_accuracy: 0.9618\n",
      "75.00% sparse random: loss: 0.13082186877727509 acc: 0.9617999792098999\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -2.6361e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -3.8534e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -4.2657e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -4.7314e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -5.3286e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -5.0003e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -6.0194e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -6.4739e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.4253e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -7.3160e+00 - 2s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1634e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.3329e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.6844e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -2.9502e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.0650e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.1530e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.3203e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.3409e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.3525e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -3.4548e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1605e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -5.6268e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -9.9905e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -1.2434e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.4511e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.5621e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.7377e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.8375e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.8955e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.9648e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3504e+00 - 712ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.2107e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4196e+00 - 435ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.5876e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.7067e+00 - 419ms/epoch - 975us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7640e+00 - 420ms/epoch - 977us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.8392e+00 - 420ms/epoch - 977us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.8956e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9862e+00 - 421ms/epoch - 979us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.0210e+00 - 424ms/epoch - 986us/step\n",
      "1719/1719 [==============================] - 1s 795us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.2305e-01 - 966ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -6.2315e-01 - 462ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -7.7640e-01 - 458ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -8.4833e-01 - 453ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -9.1100e-01 - 455ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.5460e-01 - 443ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.0068e+00 - 458ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.0335e+00 - 459ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0668e+00 - 489ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.1167e+00 - 474ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 816us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.0147e-01 - 740ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.6945e-01 - 468ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.5086e-01 - 436ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.1578e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.6276e-01 - 451ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.0488e-01 - 451ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.2473e-01 - 428ms/epoch - 996us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.4766e-01 - 417ms/epoch - 970us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.6393e-01 - 424ms/epoch - 986us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.9247e-01 - 417ms/epoch - 970us/step\n",
      "1719/1719 [==============================] - 1s 769us/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -2.5055e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.9556e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.1600e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.3238e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -4.5750e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -4.9433e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -4.9003e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -5.0125e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -5.5735e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -6.1447e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3457e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.4532e+00 - 985ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -2.8368e+00 - 824ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.1256e+00 - 799ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -3.2361e+00 - 955ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.4116e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.5267e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.5527e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.7255e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.7662e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.0197e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.3221e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -7.0581e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -8.1373e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -8.9585e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -9.7490e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.0500e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -1.1278e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -1.1732e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.3198e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3265e+00 - 745ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.2597e+00 - 451ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.5461e+00 - 448ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6983e+00 - 448ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.9000e+00 - 481ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -3.0117e+00 - 461ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -3.1024e+00 - 474ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -3.1692e+00 - 484ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.2551e+00 - 476ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.3370e+00 - 450ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 812us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.7345e-01 - 746ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.3943e-01 - 451ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -5.5989e-01 - 450ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -6.1797e-01 - 448ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -6.4815e-01 - 448ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -6.9523e-01 - 444ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -7.5222e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -8.3588e-01 - 442ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -9.0351e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -9.6856e-01 - 442ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 803us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3281e-01 - 720ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.4940e-01 - 418ms/epoch - 972us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -4.1778e-01 - 425ms/epoch - 989us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.5841e-01 - 422ms/epoch - 981us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.6577e-01 - 418ms/epoch - 972us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.9868e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.0718e-01 - 442ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.3331e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.4554e-01 - 417ms/epoch - 970us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.5863e-01 - 418ms/epoch - 972us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 1s 757us/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.6848e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -4.1417e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -4.4639e+00 - 989ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -4.6629e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -5.3599e+00 - 997ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -5.3640e+00 - 977ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.9802e+00 - 844ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -5.7971e+00 - 903ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -6.6379e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -6.9421e+00 - 1s/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.2658e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.4082e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.8446e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -2.9744e+00 - 934ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.1847e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.3386e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -3.3681e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.4054e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.5402e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.5987e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.0390e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -3.9722e-01 - 1s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -6.2804e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -7.6659e-01 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -8.6151e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -9.0674e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -9.7024e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.0293e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.0412e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.1163e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 8s 5ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1887e+00 - 712ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.1069e+00 - 449ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4432e+00 - 444ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6719e+00 - 416ms/epoch - 968us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.7462e+00 - 417ms/epoch - 970us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.9072e+00 - 414ms/epoch - 963us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.9500e+00 - 415ms/epoch - 965us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -3.0243e+00 - 415ms/epoch - 965us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.1543e+00 - 417ms/epoch - 970us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.1165e+00 - 414ms/epoch - 963us/step\n",
      "1719/1719 [==============================] - 1s 731us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.3765e-01 - 705ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.6240e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.1063e-01 - 411ms/epoch - 956us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -6.4393e-01 - 414ms/epoch - 963us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -6.8073e-01 - 414ms/epoch - 963us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -7.2555e-01 - 413ms/epoch - 960us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -7.8373e-01 - 411ms/epoch - 956us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -8.6047e-01 - 411ms/epoch - 956us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -9.0392e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -9.9282e-01 - 413ms/epoch - 961us/step\n",
      "1719/1719 [==============================] - 1s 731us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.8513e-01 - 689ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.2033e-01 - 395ms/epoch - 919us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.6246e-01 - 395ms/epoch - 919us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.2119e-01 - 404ms/epoch - 940us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.8169e-01 - 430ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.2244e-01 - 424ms/epoch - 986us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.3945e-01 - 475ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.6201e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.8002e-01 - 428ms/epoch - 995us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.8562e-01 - 421ms/epoch - 978us/step\n",
      "1719/1719 [==============================] - 1s 726us/step\n",
      "estimated mutual information x_d1, large_final: 9.845486\n",
      "estimated mutual information x_d2, large_final: 3.3177652\n",
      "estimated mutual information x_o, large_final: 1.7714429\n",
      "estimated mutual information d1_d2, large_final: 2.6533408\n",
      "estimated mutual information d1_o, large_final: 1.0826074\n",
      "estimated mutual information d2_o, large_final: 0.5796146\n",
      "estimated mutual information x_d1, large_final, init_weights: 7.5650153\n",
      "estimated mutual information x_d2, large_final, init_weights: 3.3829932\n",
      "estimated mutual information x_o, large_final, init_weights: 1.2257143\n",
      "estimated mutual information d1_d2, large_final, init_weights: 2.8384683\n",
      "estimated mutual information d1_o, large_final, init_weights: 0.9111845\n",
      "estimated mutual information d2_o, large_final, init_weights: 0.5248006\n",
      "estimated mutual information x_d1, random: 9.876525\n",
      "estimated mutual information x_d2, random: 3.5035257\n",
      "estimated mutual information x_o, random: 1.063012\n",
      "estimated mutual information d1_d2, random: 2.8744974\n",
      "estimated mutual information d1_o, random: 0.95823675\n",
      "estimated mutual information d2_o, random: 0.5864175\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, 75.00% sparse large_final, first 10 arr: \n",
      "[[-0.         -0.6555104  -0.         -0.          0.37475958  0.\n",
      "  -0.8270327   0.         -0.          0.        ]\n",
      " [-0.          0.6939685  -0.8541251   0.          0.         -0.\n",
      "  -0.58735406 -0.35565662 -0.          0.        ]\n",
      " [-0.8610594  -0.          0.         -0.         -0.          0.\n",
      "  -0.         -0.7668876  -0.          0.        ]\n",
      " [-0.          0.         -0.          0.         -0.         -0.\n",
      "  -0.47533265  0.          0.59997314  0.        ]\n",
      " [ 0.          0.          0.         -0.         -1.0371474  -0.\n",
      "  -0.          0.         -0.55049866 -0.5403507 ]\n",
      " [ 0.         -0.7606694  -0.         -0.         -0.          0.49672374\n",
      "   0.         -0.8855478   0.          0.        ]\n",
      " [-0.          0.          0.         -0.          0.         -0.5541165\n",
      "   0.50199354 -0.          0.         -0.        ]\n",
      " [ 0.          0.          0.37290594 -0.         -0.          0.\n",
      "  -0.          0.84650534 -0.5322121  -0.        ]\n",
      " [ 0.          0.          0.8343166  -0.         -0.         -0.\n",
      "   0.         -0.          0.         -0.        ]\n",
      " [ 0.4630309  -0.46004802  0.         -0.43642774 -0.          0.\n",
      "   0.         -0.          0.          0.        ]]\n",
      "output layer weight mask, 75.00% sparse large_final, init_weights, first 10 arr: \n",
      "[[ 0.16793531 -0.19815828  0.00577998  0.35408348  0.1492154   0.12632018\n",
      "   0.24829686  0.29094827 -0.28615838  0.26713395]\n",
      " [ 0.38530135  0.17435563  0.12733388 -0.372132    0.22066021  0.06476378\n",
      "   0.1588955   0.05780235  0.30895227  0.3318572 ]\n",
      " [-0.17773443 -0.24720004  0.21787155 -0.11562759 -0.17134291  0.3837418\n",
      "  -0.22211751 -0.10500547  0.05907276  0.37995923]\n",
      " [ 0.31963456 -0.27910802 -0.3465212   0.32741535 -0.2596431  -0.18591106\n",
      "   0.22975498  0.13370961 -0.09820801 -0.21896552]\n",
      " [ 0.29762203  0.12608945  0.38343257 -0.2541073  -0.10766399 -0.11768603\n",
      "  -0.220159    0.17379111 -0.31485507 -0.19066072]\n",
      " [ 0.04082459  0.10627577 -0.33504096  0.08793187  0.30454677  0.22761899\n",
      "   0.172822    0.16477382 -0.2913881   0.11035162]\n",
      " [ 0.26548386  0.01757878 -0.36683932 -0.22290719 -0.1156061  -0.16576977\n",
      "   0.01223603 -0.03303978 -0.37626788 -0.3815471 ]\n",
      " [ 0.37485212  0.22040045  0.38529146 -0.12884974 -0.32977283 -0.16270763\n",
      "  -0.14514084 -0.11418906  0.2590742   0.38547862]\n",
      " [ 0.01368871  0.05117202  0.36923832  0.22120643  0.15816945 -0.2112902\n",
      "   0.3741868   0.21973127 -0.21325877  0.0313988 ]\n",
      " [-0.06928307  0.12325382  0.26712888  0.17257756 -0.1154494   0.1961788\n",
      "  -0.32805893  0.31263125  0.3788075  -0.05266044]]\n",
      "output layer weight mask, 75.00% sparse random, first 10 arr: \n",
      "[[ 0.          0.         -0.         -0.16429465 -0.         -0.\n",
      "  -0.73423886  0.         -0.76493806  0.        ]\n",
      " [-0.          0.         -0.          0.         -1.3896728   0.\n",
      "  -0.          0.          0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.6733748   0.5685785   0.\n",
      "  -0.37872064  0.         -1.1930912  -0.28130096]\n",
      " [-0.          0.          0.62517434  0.57024354 -0.         -0.14209491\n",
      "  -0.          0.          0.57058537 -0.4712862 ]\n",
      " [ 0.          0.         -0.         -1.0042838   0.         -0.\n",
      "  -0.01731161 -0.4908032   0.         -0.        ]\n",
      " [-0.         -0.28194177 -0.27846107  0.13759746 -0.         -0.\n",
      "  -0.         -0.         -0.0409817   0.        ]\n",
      " [ 0.         -0.71335125 -0.          0.65085715 -0.          0.\n",
      "  -0.         -0.          0.         -0.3911626 ]\n",
      " [-0.         -0.19757602  0.          0.         -0.         -0.\n",
      "  -0.          0.9404682   0.         -0.        ]\n",
      " [ 0.         -0.9222142   0.          0.         -0.         -0.\n",
      "  -0.         -0.6115396  -0.          0.        ]\n",
      " [-0.         -0.         -0.          0.         -0.06476577 -0.\n",
      "  -1.1844591   0.          0.          0.18928519]]\n",
      "prune_mask calculated\n",
      "\n",
      "Iteration 3: making 87.50% sparse large_final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: making 87.50% sparse random\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3448 - sparse_categorical_accuracy: 0.8824\n",
      "87.50% sparse large_final: loss: 0.34478759765625 acc: 0.8823999762535095\n",
      "313/313 [==============================] - 0s 975us/step - loss: 2.3685 - sparse_categorical_accuracy: 0.0992\n",
      "87.50% sparse large_final, init_weights: loss: 2.3685452938079834 acc: 0.09920000284910202\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4427 - sparse_categorical_accuracy: 0.8615\n",
      "87.50% sparse random: loss: 0.442656934261322 acc: 0.8615000247955322\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.4711e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.8029e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -4.3209e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -4.2790e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -4.5333e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -5.2181e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.0528e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -5.4509e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -5.9327e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -6.6045e+00 - 1s/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 4s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.4254e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.5604e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.9528e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -3.2019e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.2397e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.4231e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.4572e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.6135e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.7120e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.8034e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.3400e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.5108e-01 - 2s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -6.8961e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -8.2737e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -9.3212e-01 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -1.0220e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -1.0886e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -1.1850e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -1.2383e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -1.3013e+00 - 3s/epoch - 7ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.4582e+00 - 701ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.2119e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4855e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6649e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.8239e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.9398e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -3.0407e+00 - 409ms/epoch - 951us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.9401e+00 - 411ms/epoch - 955us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.0563e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.1521e+00 - 408ms/epoch - 949us/step\n",
      "1719/1719 [==============================] - 1s 719us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.9257e-01 - 719ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.2816e-01 - 424ms/epoch - 986us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.6143e-01 - 423ms/epoch - 984us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.8001e-01 - 425ms/epoch - 989us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.7798e-01 - 424ms/epoch - 986us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.7182e-01 - 424ms/epoch - 986us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.0488e+00 - 423ms/epoch - 985us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.0719e+00 - 422ms/epoch - 982us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.1186e+00 - 424ms/epoch - 987us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.1597e+00 - 424ms/epoch - 986us/step\n",
      "1719/1719 [==============================] - 1s 780us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1481e-01 - 712ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.9291e-01 - 432ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.7203e-01 - 422ms/epoch - 982us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.0350e-01 - 423ms/epoch - 984us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.3133e-01 - 415ms/epoch - 964us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.4346e-01 - 410ms/epoch - 954us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -4.6016e-01 - 406ms/epoch - 943us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -4.7536e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -4.8662e-01 - 408ms/epoch - 949us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.0916e-01 - 417ms/epoch - 970us/step\n",
      "1719/1719 [==============================] - 1s 737us/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.7446e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.9832e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.6344e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -5.1400e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -5.4732e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.4051e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.9680e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -6.9445e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.2981e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -7.0969e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.1613e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -2.2566e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -2.5653e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -2.8693e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -2.9308e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.1245e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.2886e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.2865e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.3291e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.4905e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -6.6711e-02 - 2s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.4351e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -3.8161e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -5.6244e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -7.1579e-01 - 3s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -8.1644e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -9.4067e-01 - 3s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.0151e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -1.0641e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -1.1766e+00 - 3s/epoch - 7ms/step\n",
      "1719/1719 [==============================] - 7s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.4020e+00 - 712ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.3367e+00 - 414ms/epoch - 963us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.5517e+00 - 416ms/epoch - 968us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6995e+00 - 414ms/epoch - 963us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.8100e+00 - 416ms/epoch - 968us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.9236e+00 - 418ms/epoch - 972us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.8805e+00 - 414ms/epoch - 962us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -3.0035e+00 - 414ms/epoch - 963us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.0898e+00 - 413ms/epoch - 961us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.1068e+00 - 416ms/epoch - 968us/step\n",
      "1719/1719 [==============================] - 1s 728us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.7206e-01 - 700ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.8625e-01 - 407ms/epoch - 947us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.6825e-01 - 408ms/epoch - 949us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.8423e-01 - 408ms/epoch - 949us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.5322e-01 - 405ms/epoch - 942us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.2367e-01 - 406ms/epoch - 944us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -9.7574e-01 - 408ms/epoch - 949us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.0374e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0793e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.1085e+00 - 405ms/epoch - 942us/step\n",
      "1719/1719 [==============================] - 1s 726us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.5831e-01 - 688ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.6916e-01 - 393ms/epoch - 914us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -4.2043e-01 - 393ms/epoch - 914us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.3169e-01 - 395ms/epoch - 919us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.5369e-01 - 393ms/epoch - 914us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.6794e-01 - 396ms/epoch - 921us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -4.8131e-01 - 397ms/epoch - 923us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -4.9248e-01 - 395ms/epoch - 919us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.1063e-01 - 394ms/epoch - 916us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.3013e-01 - 394ms/epoch - 916us/step\n",
      "1719/1719 [==============================] - 1s 705us/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.8062e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -4.0387e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -4.6317e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.8705e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.7663e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -5.6162e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -5.8264e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -5.3659e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.7387e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -7.2775e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3449e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.4537e+00 - 957ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -2.8181e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.1195e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -3.2998e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.4776e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -3.6153e+00 - 970ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.6940e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.8684e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.7866e+00 - 988ms/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.0676e-01 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.3805e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.1203e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -9.4867e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.1312e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.3085e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.4186e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.5212e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.5633e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.6092e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2701e+00 - 720ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.1249e+00 - 432ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4417e+00 - 426ms/epoch - 991us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6811e+00 - 427ms/epoch - 993us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.7286e+00 - 427ms/epoch - 993us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.8266e+00 - 426ms/epoch - 991us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.9858e+00 - 428ms/epoch - 996us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.9475e+00 - 427ms/epoch - 993us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.0583e+00 - 427ms/epoch - 993us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.1158e+00 - 428ms/epoch - 996us/step\n",
      "1719/1719 [==============================] - 1s 743us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.4913e-01 - 712ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.5668e-01 - 422ms/epoch - 981us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -7.0433e-01 - 424ms/epoch - 986us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -9.1530e-01 - 422ms/epoch - 982us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -1.0192e+00 - 421ms/epoch - 979us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.1091e+00 - 422ms/epoch - 982us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.1877e+00 - 422ms/epoch - 982us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.2411e+00 - 421ms/epoch - 979us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.2617e+00 - 422ms/epoch - 982us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.2994e+00 - 420ms/epoch - 977us/step\n",
      "1719/1719 [==============================] - 1s 734us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -9.8043e-02 - 915ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.9850e-01 - 425ms/epoch - 987us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.5645e-01 - 423ms/epoch - 984us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.0849e-01 - 422ms/epoch - 980us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.5648e-01 - 421ms/epoch - 978us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.8126e-01 - 421ms/epoch - 979us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -4.9911e-01 - 420ms/epoch - 977us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.1311e-01 - 420ms/epoch - 977us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.2093e-01 - 423ms/epoch - 984us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.2732e-01 - 430ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 726us/step\n",
      "estimated mutual information x_d1, large_final: 7.670092\n",
      "estimated mutual information x_d2, large_final: 3.5754733\n",
      "estimated mutual information x_o, large_final: 1.3643754\n",
      "estimated mutual information d1_d2, large_final: 2.7627716\n",
      "estimated mutual information d1_o, large_final: 1.060092\n",
      "estimated mutual information d2_o, large_final: 0.49777368\n",
      "estimated mutual information x_d1, large_final, init_weights: 12.056253\n",
      "estimated mutual information x_d2, large_final, init_weights: 3.0709074\n",
      "estimated mutual information x_o, large_final, init_weights: 1.2080095\n",
      "estimated mutual information d1_d2, large_final, init_weights: 2.7783544\n",
      "estimated mutual information d1_o, large_final, init_weights: 1.0217611\n",
      "estimated mutual information d2_o, large_final, init_weights: 0.48684588\n",
      "estimated mutual information x_d1, random: 9.858416\n",
      "estimated mutual information x_d2, random: 3.8800225\n",
      "estimated mutual information x_o, random: 1.4692595\n",
      "estimated mutual information d1_d2, random: 2.7984538\n",
      "estimated mutual information d1_o, random: 1.2390413\n",
      "estimated mutual information d2_o, random: 0.5085484\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, 87.50% sparse large_final, first 10 arr: \n",
      "[[ 0.         -0.9421028  -0.          0.          0.         -0.\n",
      "  -0.9163339   0.         -0.          0.        ]\n",
      " [-0.          0.73605895 -0.9788272   0.         -0.         -0.\n",
      "  -0.          0.         -0.          0.        ]\n",
      " [-1.1705469   0.          0.          0.         -0.          0.\n",
      "   0.         -0.6886023  -0.          0.        ]\n",
      " [-0.          0.         -0.          0.          0.         -0.\n",
      "  -0.         -0.          0.         -0.        ]\n",
      " [ 0.          0.          0.          0.         -1.0230482   0.\n",
      "  -0.          0.         -0.         -0.        ]\n",
      " [ 0.         -0.80322534 -0.          0.         -0.          0.\n",
      "  -0.         -0.80820364 -0.          0.        ]\n",
      " [-0.          0.         -0.          0.          0.         -0.\n",
      "   0.         -0.         -0.          0.        ]\n",
      " [-0.          0.          0.         -0.         -0.          0.\n",
      "   0.          0.87096494 -0.         -0.        ]\n",
      " [ 0.          0.          0.8958902  -0.         -0.          0.\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [ 0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.          0.         -0.         -0.        ]]\n",
      "output layer weight mask, 87.50% sparse large_final, init_weights, first 10 arr: \n",
      "[[ 0.16793531 -0.19815828  0.00577998  0.35408348  0.1492154   0.12632018\n",
      "   0.24829686  0.29094827 -0.28615838  0.26713395]\n",
      " [ 0.38530135  0.17435563  0.12733388 -0.372132    0.22066021  0.06476378\n",
      "   0.1588955   0.05780235  0.30895227  0.3318572 ]\n",
      " [-0.17773443 -0.24720004  0.21787155 -0.11562759 -0.17134291  0.3837418\n",
      "  -0.22211751 -0.10500547  0.05907276  0.37995923]\n",
      " [ 0.31963456 -0.27910802 -0.3465212   0.32741535 -0.2596431  -0.18591106\n",
      "   0.22975498  0.13370961 -0.09820801 -0.21896552]\n",
      " [ 0.29762203  0.12608945  0.38343257 -0.2541073  -0.10766399 -0.11768603\n",
      "  -0.220159    0.17379111 -0.31485507 -0.19066072]\n",
      " [ 0.04082459  0.10627577 -0.33504096  0.08793187  0.30454677  0.22761899\n",
      "   0.172822    0.16477382 -0.2913881   0.11035162]\n",
      " [ 0.26548386  0.01757878 -0.36683932 -0.22290719 -0.1156061  -0.16576977\n",
      "   0.01223603 -0.03303978 -0.37626788 -0.3815471 ]\n",
      " [ 0.37485212  0.22040045  0.38529146 -0.12884974 -0.32977283 -0.16270763\n",
      "  -0.14514084 -0.11418906  0.2590742   0.38547862]\n",
      " [ 0.01368871  0.05117202  0.36923832  0.22120643  0.15816945 -0.2112902\n",
      "   0.3741868   0.21973127 -0.21325877  0.0313988 ]\n",
      " [-0.06928307  0.12325382  0.26712888  0.17257756 -0.1154494   0.1961788\n",
      "  -0.32805893  0.31263125  0.3788075  -0.05266044]]\n",
      "output layer weight mask, 87.50% sparse random, first 10 arr: \n",
      "[[-0.          0.         -0.         -0.22099829  0.         -0.\n",
      "  -1.1291561   0.         -0.          0.        ]\n",
      " [ 0.          0.         -0.          0.         -1.5202401   0.\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.8330478   0.         -0.\n",
      "   0.         -0.          0.         -0.7607925 ]\n",
      " [-0.         -0.          0.4056462   0.          0.         -1.1551325\n",
      "  -0.         -0.          0.6315867  -0.01254443]\n",
      " [-0.         -0.         -0.         -0.7273354   0.         -0.\n",
      "   0.         -0.8172033   0.          0.        ]\n",
      " [-0.          0.         -0.         -0.          0.         -0.\n",
      "  -0.          0.         -0.          0.        ]\n",
      " [-0.          0.         -0.          1.1713662  -0.          0.\n",
      "  -0.         -0.         -0.          0.        ]\n",
      " [-0.          0.          0.          0.         -0.         -0.\n",
      "   0.          0.          0.         -0.        ]\n",
      " [-0.          0.         -0.         -0.          0.          0.\n",
      "   0.         -1.1202832   0.         -0.        ]\n",
      " [-0.          0.         -0.          0.         -0.16065893  0.\n",
      "  -1.3129599  -0.         -0.          0.        ]]\n",
      "prune_mask calculated\n",
      "\n",
      "Iteration 4: making 93.75% sparse large_final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: making 93.75% sparse random\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 763us/step - loss: 0.9894 - sparse_categorical_accuracy: 0.6310\n",
      "93.75% sparse large_final: loss: 0.9894002079963684 acc: 0.6309999823570251\n",
      "313/313 [==============================] - 0s 750us/step - loss: 2.3685 - sparse_categorical_accuracy: 0.0992\n",
      "93.75% sparse large_final, init_weights: loss: 2.3685452938079834 acc: 0.09920000284910202\n",
      "313/313 [==============================] - 0s 757us/step - loss: 1.4677 - sparse_categorical_accuracy: 0.4831\n",
      "93.75% sparse random: loss: 1.4676746129989624 acc: 0.4830999970436096\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.5755e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -4.0574e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.3954e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.8320e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.9885e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -4.9404e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.5002e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -5.8170e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -5.9567e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -7.2510e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.4986e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.8135e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -3.0554e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -3.3710e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.5638e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -3.6785e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -3.6658e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.7400e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 4s - loss: -3.8994e+00 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -4.0416e+00 - 3s/epoch - 7ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -7.0304e-02 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.3567e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -3.9824e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -5.3037e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -6.5619e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -7.5553e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -8.3842e-01 - 1s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -9.1548e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -9.5945e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.0345e+00 - 1s/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2116e+00 - 700ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.0496e+00 - 412ms/epoch - 958us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.2983e+00 - 407ms/epoch - 947us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.4800e+00 - 410ms/epoch - 954us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.5550e+00 - 428ms/epoch - 996us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7052e+00 - 425ms/epoch - 989us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.7734e+00 - 408ms/epoch - 949us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.8206e+00 - 421ms/epoch - 979us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.8773e+00 - 410ms/epoch - 954us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.9608e+00 - 410ms/epoch - 954us/step\n",
      "1719/1719 [==============================] - 1s 777us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.1646e-01 - 700ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.6115e-01 - 405ms/epoch - 942us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.8307e-01 - 401ms/epoch - 933us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.1802e-01 - 402ms/epoch - 935us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -7.5535e-01 - 405ms/epoch - 942us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -8.2989e-01 - 404ms/epoch - 940us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -9.1457e-01 - 403ms/epoch - 937us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -9.5752e-01 - 402ms/epoch - 935us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0185e+00 - 405ms/epoch - 942us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.0514e+00 - 438ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 776us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.8513e-01 - 734ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.0758e-01 - 433ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -5.2389e-01 - 421ms/epoch - 979us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -6.0074e-01 - 417ms/epoch - 970us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -6.5472e-01 - 415ms/epoch - 965us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -6.9744e-01 - 413ms/epoch - 961us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -7.3385e-01 - 413ms/epoch - 961us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -7.7570e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -7.9339e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -8.3911e-01 - 412ms/epoch - 958us/step\n",
      "1719/1719 [==============================] - 1s 783us/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.8060e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.1635e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -4.4943e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -4.7623e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -5.0894e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -5.2505e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.6253e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -6.0782e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -7.3460e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -7.1339e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 6s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3753e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.4646e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.7994e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -2.9602e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.2180e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.3475e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.4601e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.4490e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.6352e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.6161e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -8.6482e-02 - 2s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.3733e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -5.1789e-01 - 2s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -7.5497e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -9.1235e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.0321e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.0789e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.1740e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.2415e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.3328e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1013e+00 - 728ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -1.8891e+00 - 441ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.2151e+00 - 431ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.3532e+00 - 433ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.4903e+00 - 456ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.6927e+00 - 460ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.7799e+00 - 456ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.8171e+00 - 459ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9075e+00 - 494ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.9048e+00 - 472ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 831us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2257e-01 - 756ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.8871e-01 - 466ms/epoch - 1ms/step\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 - 0s - loss: -5.7944e-01 - 462ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.3944e-01 - 464ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.4715e-01 - 462ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.3349e-01 - 457ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -9.8177e-01 - 474ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -9.9972e-01 - 448ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0645e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.0865e+00 - 432ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 813us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2850e-01 - 728ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.5422e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.0175e-01 - 413ms/epoch - 961us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -3.5092e-01 - 415ms/epoch - 965us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -3.7266e-01 - 414ms/epoch - 963us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -3.9576e-01 - 412ms/epoch - 959us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -4.0062e-01 - 417ms/epoch - 970us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -4.2316e-01 - 414ms/epoch - 963us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -4.2942e-01 - 415ms/epoch - 965us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -4.4078e-01 - 431ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 756us/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -2.8818e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -4.4285e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.7116e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -4.8239e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -5.2249e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.4584e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -5.7603e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -6.9918e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -6.6766e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -7.1531e+00 - 2s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 7s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.4358e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.6014e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -3.0248e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.2685e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.4157e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.5888e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.7028e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.7708e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.9818e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.9223e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 7s 4ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.2481e-01 - 2s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.0492e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.0128e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -9.1160e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.0419e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1959e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -1.3133e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.3856e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.4739e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.5520e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2526e+00 - 752ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.0796e+00 - 412ms/epoch - 958us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.3003e+00 - 454ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.5030e+00 - 458ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.6099e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7716e+00 - 441ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.7066e+00 - 479ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.8531e+00 - 470ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9055e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.9466e+00 - 436ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 796us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.1780e-01 - 738ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -6.5659e-01 - 458ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -8.3188e-01 - 434ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -8.8900e-01 - 419ms/epoch - 975us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -9.2616e-01 - 422ms/epoch - 982us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.7938e-01 - 430ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.0147e+00 - 424ms/epoch - 986us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.0662e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.1499e+00 - 414ms/epoch - 963us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.1791e+00 - 423ms/epoch - 984us/step\n",
      "1719/1719 [==============================] - 1s 775us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.4362e-01 - 963ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.9891e-01 - 472ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.8911e-01 - 461ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.7268e-01 - 457ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -5.3118e-01 - 460ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.9560e-01 - 476ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -6.4349e-01 - 455ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -6.7059e-01 - 471ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -6.9497e-01 - 471ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -7.2443e-01 - 461ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 777us/step\n",
      "estimated mutual information x_d1, large_final: 9.661241\n",
      "estimated mutual information x_d2, large_final: 3.9822764\n",
      "estimated mutual information x_o, large_final: 1.0183946\n",
      "estimated mutual information d1_d2, large_final: 2.5857282\n",
      "estimated mutual information d1_o, large_final: 0.9929682\n",
      "estimated mutual information d2_o, large_final: 0.7359847\n",
      "estimated mutual information x_d1, large_final, init_weights: 10.759013\n",
      "estimated mutual information x_d2, large_final, init_weights: 3.2577052\n",
      "estimated mutual information x_o, large_final, init_weights: 1.2755748\n",
      "estimated mutual information d1_d2, large_final, init_weights: 2.685001\n",
      "estimated mutual information d1_o, large_final, init_weights: 1.0513512\n",
      "estimated mutual information d2_o, large_final, init_weights: 0.421503\n",
      "estimated mutual information x_d1, random: 10.993747\n",
      "estimated mutual information x_d2, random: 3.946771\n",
      "estimated mutual information x_o, random: 1.3186303\n",
      "estimated mutual information d1_d2, random: 2.6592171\n",
      "estimated mutual information d1_o, random: 1.1782439\n",
      "estimated mutual information d2_o, random: 0.6440117\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, 93.75% sparse large_final, first 10 arr: \n",
      "[[ 0.        -1.1161461 -0.        -0.         0.         0.\n",
      "  -0.         0.        -0.         0.       ]\n",
      " [-0.         0.        -1.2117686  0.         0.         0.\n",
      "  -0.        -0.        -0.         0.       ]\n",
      " [-1.293252  -0.         0.        -0.        -0.         0.\n",
      "   0.        -0.         0.         0.       ]\n",
      " [-0.        -0.         0.        -0.        -0.         0.\n",
      "   0.        -0.         0.         0.       ]\n",
      " [ 0.         0.         0.        -0.        -1.2111697  0.\n",
      "   0.         0.        -0.        -0.       ]\n",
      " [ 0.         0.         0.        -0.         0.         0.\n",
      "   0.        -0.         0.         0.       ]\n",
      " [ 0.         0.         0.        -0.         0.         0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [-0.         0.         0.         0.        -0.        -0.\n",
      "  -0.         0.        -0.        -0.       ]\n",
      " [ 0.         0.         0.        -0.        -0.         0.\n",
      "   0.         0.         0.        -0.       ]\n",
      " [ 0.         0.         0.        -0.         0.         0.\n",
      "   0.        -0.         0.         0.       ]]\n",
      "output layer weight mask, 93.75% sparse large_final, init_weights, first 10 arr: \n",
      "[[ 0.16793531 -0.19815828  0.00577998  0.35408348  0.1492154   0.12632018\n",
      "   0.24829686  0.29094827 -0.28615838  0.26713395]\n",
      " [ 0.38530135  0.17435563  0.12733388 -0.372132    0.22066021  0.06476378\n",
      "   0.1588955   0.05780235  0.30895227  0.3318572 ]\n",
      " [-0.17773443 -0.24720004  0.21787155 -0.11562759 -0.17134291  0.3837418\n",
      "  -0.22211751 -0.10500547  0.05907276  0.37995923]\n",
      " [ 0.31963456 -0.27910802 -0.3465212   0.32741535 -0.2596431  -0.18591106\n",
      "   0.22975498  0.13370961 -0.09820801 -0.21896552]\n",
      " [ 0.29762203  0.12608945  0.38343257 -0.2541073  -0.10766399 -0.11768603\n",
      "  -0.220159    0.17379111 -0.31485507 -0.19066072]\n",
      " [ 0.04082459  0.10627577 -0.33504096  0.08793187  0.30454677  0.22761899\n",
      "   0.172822    0.16477382 -0.2913881   0.11035162]\n",
      " [ 0.26548386  0.01757878 -0.36683932 -0.22290719 -0.1156061  -0.16576977\n",
      "   0.01223603 -0.03303978 -0.37626788 -0.3815471 ]\n",
      " [ 0.37485212  0.22040045  0.38529146 -0.12884974 -0.32977283 -0.16270763\n",
      "  -0.14514084 -0.11418906  0.2590742   0.38547862]\n",
      " [ 0.01368871  0.05117202  0.36923832  0.22120643  0.15816945 -0.2112902\n",
      "   0.3741868   0.21973127 -0.21325877  0.0313988 ]\n",
      " [-0.06928307  0.12325382  0.26712888  0.17257756 -0.1154494   0.1961788\n",
      "  -0.32805893  0.31263125  0.3788075  -0.05266044]]\n",
      "output layer weight mask, 93.75% sparse random, first 10 arr: \n",
      "[[ 0.         -0.          0.         -0.          0.         -0.\n",
      "   0.         -0.         -0.          0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.37294775  0.\n",
      "   0.         -0.         -0.         -0.        ]\n",
      " [ 0.          0.          0.          0.         -0.         -0.\n",
      "  -0.         -0.          0.         -0.87513477]\n",
      " [-0.          0.          0.         -0.          0.         -0.\n",
      "  -0.          0.          0.868901    0.4427584 ]\n",
      " [-0.          0.         -0.          0.          0.          0.\n",
      "  -0.         -0.          0.          0.        ]\n",
      " [-0.          0.          0.         -0.          0.          0.\n",
      "  -0.          0.         -0.          0.        ]\n",
      " [ 0.         -0.         -0.         -0.          0.          0.\n",
      "   0.         -0.          0.         -0.        ]\n",
      " [-0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.          0.         -0.        ]\n",
      " [ 0.          0.         -0.         -0.          0.          0.\n",
      "   0.         -0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -1.3774347   0.\n",
      "  -0.         -0.         -0.         -0.        ]]\n",
      "prune_mask calculated\n",
      "\n",
      "Iteration 5: making 96.88% sparse large_final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: making 96.88% sparse random\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 862us/step - loss: 1.9035 - sparse_categorical_accuracy: 0.2575\n",
      "96.88% sparse large_final: loss: 1.9035412073135376 acc: 0.2574999928474426\n",
      "313/313 [==============================] - 0s 795us/step - loss: 2.3685 - sparse_categorical_accuracy: 0.0992\n",
      "96.88% sparse large_final, init_weights: loss: 2.3685452938079834 acc: 0.09920000284910202\n",
      "313/313 [==============================] - 0s 794us/step - loss: 2.1926 - sparse_categorical_accuracy: 0.2167\n",
      "96.88% sparse random: loss: 2.1925604343414307 acc: 0.2167000025510788\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.5076e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -4.0523e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.3576e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.8773e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.9944e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.3780e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.8352e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -6.3212e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -7.3528e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -6.7531e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1662e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.3526e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.7682e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -2.9749e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.0460e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.2550e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.2969e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.4422e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.4602e+00 - 849ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.4836e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1974e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.2040e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.1767e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -9.2542e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -1.0290e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1636e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.2699e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -1.3337e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.3988e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.4654e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1485e+00 - 737ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.0990e+00 - 474ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.3969e+00 - 471ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6134e+00 - 460ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.6727e+00 - 444ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7816e+00 - 450ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.8242e+00 - 442ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.9251e+00 - 437ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.0284e+00 - 437ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.0235e+00 - 434ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 758us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.0361e-01 - 746ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.6578e-01 - 455ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -7.1888e-01 - 453ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -8.0326e-01 - 452ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.8292e-01 - 451ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.4251e-01 - 452ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.0241e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.0799e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.1395e+00 - 452ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.2132e+00 - 464ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 765us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3478e-01 - 715ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.9790e-01 - 420ms/epoch - 977us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -4.9668e-01 - 422ms/epoch - 981us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -5.3681e-01 - 423ms/epoch - 984us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -5.6333e-01 - 422ms/epoch - 982us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.8007e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.7675e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.9779e-01 - 420ms/epoch - 977us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -6.0590e-01 - 421ms/epoch - 980us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.1910e-01 - 420ms/epoch - 977us/step\n",
      "1719/1719 [==============================] - 1s 741us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.8113e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -4.0501e+00 - 946ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -4.4134e+00 - 964ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -4.5467e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -4.8018e+00 - 969ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -5.1063e+00 - 671ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.6652e+00 - 707ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -5.8115e+00 - 935ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -7.1365e+00 - 869ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -7.6657e+00 - 678ms/epoch - 2ms/step\n",
      "1719/1719 [==============================] - 6s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -1.4060e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 3s - loss: -2.5667e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.9127e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -3.1185e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.3081e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.5087e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -3.6681e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -3.7079e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.8195e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -3.8427e+00 - 3s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 6s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1559e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.8180e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -6.8297e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -8.9315e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.0470e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1873e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.3288e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.3881e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.4908e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.6017e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.4191e+00 - 773ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.3023e+00 - 456ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.4967e+00 - 458ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.6594e+00 - 465ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.7713e+00 - 466ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.8324e+00 - 497ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.9293e+00 - 451ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.9523e+00 - 463ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9862e+00 - 451ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.0424e+00 - 450ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 800us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.3741e-01 - 752ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.7804e-01 - 468ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.5950e-01 - 495ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.6821e-01 - 468ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.3410e-01 - 492ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -9.2386e-01 - 479ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -9.8729e-01 - 462ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.0588e+00 - 449ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.1306e+00 - 463ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.2040e+00 - 482ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 852us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1797e-01 - 727ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.0883e-01 - 416ms/epoch - 968us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -4.0192e-01 - 428ms/epoch - 996us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.3636e-01 - 461ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.6555e-01 - 439ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.9016e-01 - 427ms/epoch - 993us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.2078e-01 - 414ms/epoch - 963us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.3512e-01 - 439ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.5420e-01 - 432ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -5.8170e-01 - 425ms/epoch - 989us/step\n",
      "1719/1719 [==============================] - 1s 734us/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.8267e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.1148e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.6386e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.6775e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.9834e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -5.1303e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.9128e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -5.9993e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -6.1261e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -7.3525e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3977e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.6059e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -3.0174e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -3.2392e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.4086e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.5322e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.5751e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.7443e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.8478e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.8063e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -9.8066e-02 - 2s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.6760e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -6.4910e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -8.5707e-01 - 3s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -9.6343e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.0714e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.1766e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -1.2601e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -1.3410e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.4451e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.0573e+00 - 728ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -1.8358e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.2185e+00 - 425ms/epoch - 989us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.4403e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.5924e+00 - 428ms/epoch - 996us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7200e+00 - 436ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.7726e+00 - 454ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.8765e+00 - 450ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.8765e+00 - 443ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.9378e+00 - 440ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 785us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.8295e-01 - 726ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -6.3607e-01 - 420ms/epoch - 977us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -9.0974e-01 - 404ms/epoch - 940us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -1.0594e+00 - 406ms/epoch - 945us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -1.1310e+00 - 404ms/epoch - 940us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.1768e+00 - 427ms/epoch - 993us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.2266e+00 - 416ms/epoch - 968us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.2345e+00 - 410ms/epoch - 954us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.2698e+00 - 423ms/epoch - 984us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.2723e+00 - 429ms/epoch - 998us/step\n",
      "1719/1719 [==============================] - 1s 771us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.7965e-01 - 712ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.7006e-01 - 434ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.9460e-01 - 442ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.3262e-01 - 420ms/epoch - 977us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.7107e-01 - 443ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.0937e-01 - 423ms/epoch - 984us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.3763e-01 - 428ms/epoch - 996us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.6553e-01 - 426ms/epoch - 991us/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -5.8042e-01 - 427ms/epoch - 993us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.0228e-01 - 446ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 752us/step\n",
      "estimated mutual information x_d1, large_final: 10.607498\n",
      "estimated mutual information x_d2, large_final: 3.4585044\n",
      "estimated mutual information x_o, large_final: 1.16047\n",
      "estimated mutual information d1_d2, large_final: 2.7415571\n",
      "estimated mutual information d1_o, large_final: 1.1135155\n",
      "estimated mutual information d2_o, large_final: 0.56830066\n",
      "estimated mutual information x_d1, large_final, init_weights: 11.244976\n",
      "estimated mutual information x_d2, large_final, init_weights: 3.6562858\n",
      "estimated mutual information x_o, large_final, init_weights: 1.5334798\n",
      "estimated mutual information d1_d2, large_final, init_weights: 2.8409774\n",
      "estimated mutual information d1_o, large_final, init_weights: 1.132687\n",
      "estimated mutual information d2_o, large_final, init_weights: 0.55817187\n",
      "estimated mutual information x_d1, random: 9.528653\n",
      "estimated mutual information x_d2, random: 3.4497504\n",
      "estimated mutual information x_o, random: 1.3460875\n",
      "estimated mutual information d1_d2, random: 2.713395\n",
      "estimated mutual information d1_o, random: 1.1653128\n",
      "estimated mutual information d2_o, random: 0.5879062\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, 96.88% sparse large_final, first 10 arr: \n",
      "[[ 0.        -0.         0.        -0.         0.         0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [-0.        -0.        -1.2239832  0.         0.         0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [-1.3049093 -0.        -0.        -0.         0.         0.\n",
      "  -0.        -0.         0.         0.       ]\n",
      " [ 0.         0.        -0.         0.        -0.        -0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [ 0.        -0.         0.         0.        -1.6872638  0.\n",
      "  -0.         0.        -0.        -0.       ]\n",
      " [ 0.        -0.         0.        -0.         0.         0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [-0.         0.        -0.         0.        -0.        -0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [-0.         0.         0.        -0.        -0.        -0.\n",
      "  -0.         0.        -0.        -0.       ]\n",
      " [-0.         0.        -0.         0.        -0.        -0.\n",
      "   0.         0.         0.         0.       ]\n",
      " [ 0.        -0.        -0.        -0.         0.         0.\n",
      "   0.        -0.         0.         0.       ]]\n",
      "output layer weight mask, 96.88% sparse large_final, init_weights, first 10 arr: \n",
      "[[ 0.16793531 -0.19815828  0.00577998  0.35408348  0.1492154   0.12632018\n",
      "   0.24829686  0.29094827 -0.28615838  0.26713395]\n",
      " [ 0.38530135  0.17435563  0.12733388 -0.372132    0.22066021  0.06476378\n",
      "   0.1588955   0.05780235  0.30895227  0.3318572 ]\n",
      " [-0.17773443 -0.24720004  0.21787155 -0.11562759 -0.17134291  0.3837418\n",
      "  -0.22211751 -0.10500547  0.05907276  0.37995923]\n",
      " [ 0.31963456 -0.27910802 -0.3465212   0.32741535 -0.2596431  -0.18591106\n",
      "   0.22975498  0.13370961 -0.09820801 -0.21896552]\n",
      " [ 0.29762203  0.12608945  0.38343257 -0.2541073  -0.10766399 -0.11768603\n",
      "  -0.220159    0.17379111 -0.31485507 -0.19066072]\n",
      " [ 0.04082459  0.10627577 -0.33504096  0.08793187  0.30454677  0.22761899\n",
      "   0.172822    0.16477382 -0.2913881   0.11035162]\n",
      " [ 0.26548386  0.01757878 -0.36683932 -0.22290719 -0.1156061  -0.16576977\n",
      "   0.01223603 -0.03303978 -0.37626788 -0.3815471 ]\n",
      " [ 0.37485212  0.22040045  0.38529146 -0.12884974 -0.32977283 -0.16270763\n",
      "  -0.14514084 -0.11418906  0.2590742   0.38547862]\n",
      " [ 0.01368871  0.05117202  0.36923832  0.22120643  0.15816945 -0.2112902\n",
      "   0.3741868   0.21973127 -0.21325877  0.0313988 ]\n",
      " [-0.06928307  0.12325382  0.26712888  0.17257756 -0.1154494   0.1961788\n",
      "  -0.32805893  0.31263125  0.3788075  -0.05266044]]\n",
      "output layer weight mask, 96.88% sparse random, first 10 arr: \n",
      "[[-0.         0.        -0.        -0.         0.         0.\n",
      "   0.        -0.         0.        -0.       ]\n",
      " [-0.         0.         0.         0.        -0.         0.\n",
      "   0.         0.         0.        -0.       ]\n",
      " [-0.         0.         0.        -0.        -0.         0.\n",
      "   0.         0.        -0.        -0.       ]\n",
      " [-0.         0.         0.        -0.         0.         0.\n",
      "  -0.        -0.         0.7923213  0.5342611]\n",
      " [-0.         0.        -0.         0.         0.         0.\n",
      "  -0.         0.         0.        -0.       ]\n",
      " [ 0.         0.        -0.         0.         0.         0.\n",
      "   0.         0.        -0.        -0.       ]\n",
      " [ 0.         0.         0.        -0.        -0.        -0.\n",
      "  -0.         0.        -0.        -0.       ]\n",
      " [ 0.         0.        -0.         0.        -0.         0.\n",
      "   0.        -0.         0.        -0.       ]\n",
      " [ 0.         0.         0.        -0.         0.         0.\n",
      "   0.        -0.         0.        -0.       ]\n",
      " [ 0.         0.         0.         0.        -0.         0.\n",
      "  -0.        -0.        -0.        -0.       ]]\n",
      "Experiment 1 saved in Dictionary\n",
      "------------------------\n",
      "------------------------\n",
      "------------------------\n",
      "Experimental run number: 2\n",
      "------------------------\n",
      "------------------------\n",
      "------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 821us/step - loss: 2.3140 - sparse_categorical_accuracy: 0.0917\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -2.6617e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.9322e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -4.2229e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.7931e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.9719e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.3939e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.7843e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -6.2692e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -5.8176e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -6.7403e+00 - 2s/epoch - 6ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2779e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.4803e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -2.8801e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.0731e+00 - 963ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -3.2362e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -3.2799e+00 - 1s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -3.3473e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -3.5238e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.4577e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.6193e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 5s 3ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 4s - loss: -1.2528e-01 - 4s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 4s - loss: -5.4057e-01 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 3s - loss: -8.8181e-01 - 3s/epoch - 8ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 3s - loss: -1.1051e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 3s - loss: -1.2175e+00 - 3s/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 3s - loss: -1.3190e+00 - 3s/epoch - 8ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 3s - loss: -1.4085e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 3s - loss: -1.4730e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 3s - loss: -1.5588e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 3s - loss: -1.6005e+00 - 3s/epoch - 7ms/step\n",
      "1719/1719 [==============================] - 4s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1101e+00 - 959ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -1.9524e+00 - 484ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -2.1540e+00 - 501ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -2.3037e+00 - 504ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -2.4499e+00 - 503ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -2.5632e+00 - 512ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -2.6885e+00 - 501ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -2.7245e+00 - 502ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -2.8521e+00 - 502ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -2.9343e+00 - 500ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 836us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -2.0202e-01 - 806ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -6.0069e-01 - 508ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -7.6373e-01 - 499ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -8.4569e-01 - 515ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.7807e-01 - 498ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -8.9609e-01 - 510ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -9.6112e-01 - 497ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -9.8419e-01 - 512ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0129e+00 - 500ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.0778e+00 - 497ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 820us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.4847e-01 - 808ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.6451e-01 - 507ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.2692e-01 - 492ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -3.9452e-01 - 500ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -4.5381e-01 - 524ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.9498e-01 - 499ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -5.3662e-01 - 507ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.6381e-01 - 496ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -5.8658e-01 - 506ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.2414e-01 - 499ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 794us/step\n",
      "estimated mutual information x_d1: 9.62086\n",
      "estimated mutual information x_d2: 3.4701397\n",
      "estimated mutual information x_o: 1.5400816\n",
      "estimated mutual information d1_d2: 2.5537026\n",
      "estimated mutual information d1_o: 1.0267986\n",
      "estimated mutual information d2_o: 0.5866281\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "fully connected model, pre-train: loss: 2.3140008449554443 acc: 0.0917000025510788\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, pre-train, first 10 arr: \n",
      "[[ 0.10372657 -0.23062739  0.17682165 -0.08873466  0.23557895 -0.03147158\n",
      "  -0.23462272  0.2651338  -0.12489226 -0.03261241]\n",
      " [-0.09772784  0.3317693  -0.33232167 -0.00817764 -0.12354863  0.06477183\n",
      "  -0.37804726 -0.2229637   0.386456   -0.13857591]\n",
      " [-0.3368967  -0.1321938  -0.1171802  -0.16798306  0.21808767  0.15431464\n",
      "  -0.21958955 -0.27966002 -0.30258098 -0.09193829]\n",
      " [-0.3046082  -0.33831993  0.37752926  0.2601511  -0.04892328 -0.06275451\n",
      "  -0.0382643   0.30578887  0.26934433  0.17941535]\n",
      " [ 0.15212536  0.24618846 -0.18440057 -0.27350062 -0.24141666 -0.20426965\n",
      "  -0.09346464 -0.20180817 -0.24661073 -0.22613518]\n",
      " [-0.03369612 -0.18294892 -0.28426045  0.2934637  -0.1341235   0.3308233\n",
      "   0.11271024 -0.15089689  0.08726564 -0.33876473]\n",
      " [-0.1912756  -0.08345768  0.1534971   0.38107044  0.15828174 -0.16249561\n",
      "   0.29039502 -0.32429057  0.2932816  -0.04296836]\n",
      " [ 0.18538827  0.07226792  0.00952986 -0.21432622  0.10683995  0.05759257\n",
      "   0.30292875  0.3747828  -0.3317164   0.09456533]\n",
      " [-0.09649152 -0.2496497   0.2983873  -0.18555251  0.12270099  0.24197662\n",
      "  -0.31861964 -0.31165904 -0.37963566 -0.07235166]\n",
      " [ 0.3217913  -0.02085996 -0.16163169 -0.12485653 -0.02415416  0.1396634\n",
      "  -0.37486386  0.06521994  0.26913399  0.13854533]]\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 862us/step - loss: 0.0904 - sparse_categorical_accuracy: 0.9757\n",
      "fully connected model, trained: loss: 0.0904313251376152 acc: 0.9757000207901001\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "early append of _init done\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.8306e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.2334e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.3892e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.8454e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.9306e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.0839e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -6.4657e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -6.1925e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.0716e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -7.3922e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 2s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.4488e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.5888e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -3.0304e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -3.2108e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.3665e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.5942e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.6537e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.7271e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.8080e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.8790e+00 - 2s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -7.6181e-02 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.8087e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -5.3143e-01 - 2s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -7.0243e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -8.6765e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -9.7313e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.0699e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.1576e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.2776e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.3286e+00 - 1s/epoch - 3ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1902e+00 - 795ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.0206e+00 - 487ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.2985e+00 - 465ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.4320e+00 - 484ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -2.6228e+00 - 503ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -2.7466e+00 - 490ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -2.7380e+00 - 499ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -2.8750e+00 - 483ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -2.9668e+00 - 496ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -2.9323e+00 - 496ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 813us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.6862e-01 - 771ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.8776e-01 - 453ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -6.7038e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -7.5719e-01 - 452ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -8.1785e-01 - 470ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -8.5320e-01 - 453ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -9.2194e-01 - 445ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -9.5995e-01 - 475ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.0273e+00 - 446ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.0801e+00 - 436ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 806us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.6676e-01 - 731ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -3.6865e-01 - 435ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -4.2548e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.4693e-01 - 412ms/epoch - 958us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.8093e-01 - 405ms/epoch - 942us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.2352e-01 - 406ms/epoch - 945us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.7937e-01 - 439ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -6.3373e-01 - 433ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -6.6985e-01 - 415ms/epoch - 965us/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.9495e-01 - 410ms/epoch - 952us/step\n",
      "1719/1719 [==============================] - 1s 753us/step\n",
      "estimated mutual information x_d1: 9.222637\n",
      "estimated mutual information x_d2: 3.8148015\n",
      "estimated mutual information x_o: 1.2613144\n",
      "estimated mutual information d1_d2: 2.5995934\n",
      "estimated mutual information d1_o: 0.9738889\n",
      "estimated mutual information d2_o: 0.6678503\n",
      "mutual info appended\n",
      "fit_loss_history appended\n",
      "\n",
      "\n",
      "sanity check\n",
      "output layer weight mask, trained, fully connected model, first 10 arr: \n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "prune_mask calculated\n",
      "\n",
      "Iteration 1: making 50.00% sparse large_final\n",
      "Iteration 1: making 50.00% sparse random\n",
      "\n",
      "\n",
      "313/313 [==============================] - 0s 894us/step - loss: 0.0769 - sparse_categorical_accuracy: 0.9756\n",
      "50.00% sparse large_final: loss: 0.07692041993141174 acc: 0.975600004196167\n",
      "313/313 [==============================] - 0s 817us/step - loss: 2.3340 - sparse_categorical_accuracy: 0.0862\n",
      "50.00% sparse large_final, init_weights: loss: 2.3340375423431396 acc: 0.08619999885559082\n",
      "313/313 [==============================] - 0s 830us/step - loss: 0.0867 - sparse_categorical_accuracy: 0.9750\n",
      "50.00% sparse random: loss: 0.08670170605182648 acc: 0.9750000238418579\n",
      "\n",
      "\n",
      "acc appended\n",
      "loss appended\n",
      "\n",
      "\n",
      "attempting mutual information neural estimation\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.7389e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.0621e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.4042e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.7211e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.7627e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -4.8822e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.7906e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -5.7119e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -6.1146e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -6.6660e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.4095e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.5072e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.8468e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -3.1554e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.2626e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.3481e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.5239e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.6940e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.6991e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.7835e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1560e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.4501e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -7.1283e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -9.3924e-01 - 1s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.1021e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1941e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -1.3143e+00 - 1s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.3998e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.4440e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.5000e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.1677e+00 - 803ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 1s - loss: -2.0370e+00 - 516ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 1s - loss: -2.2780e+00 - 517ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 1s - loss: -2.4957e+00 - 517ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 1s - loss: -2.6330e+00 - 517ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -2.7857e+00 - 515ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -2.8509e+00 - 514ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -2.9875e+00 - 514ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -3.0113e+00 - 519ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -3.0174e+00 - 530ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 2s 828us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.8479e-01 - 784ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -5.9373e-01 - 495ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -8.1668e-01 - 490ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -9.1880e-01 - 494ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -1.0082e+00 - 488ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 1s - loss: -1.0498e+00 - 503ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 1s - loss: -1.1055e+00 - 510ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 1s - loss: -1.1300e+00 - 505ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 1s - loss: -1.1661e+00 - 536ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 1s - loss: -1.2023e+00 - 534ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 827us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.3107e-01 - 772ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.9004e-01 - 454ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -3.9457e-01 - 450ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -4.5096e-01 - 447ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.9497e-01 - 447ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -5.2452e-01 - 447ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.5290e-01 - 448ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.9178e-01 - 447ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -6.1906e-01 - 446ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.5394e-01 - 447ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 760us/step\n",
      "Epoch 1/10\n",
      "430/430 - 3s - loss: -2.5348e+00 - 3s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.1076e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.2617e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.7372e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -5.1375e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.2239e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.8736e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -5.9981e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.4070e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -6.9405e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.3909e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.6756e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.9592e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -3.1520e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -3.2765e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.5122e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.4290e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.6555e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.6943e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.8192e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 2s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.0342e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.1383e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -6.7184e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -8.7953e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -1.0132e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1284e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.1942e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.3063e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.3737e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.3946e+00 - 2s/epoch - 5ms/step\n",
      "1719/1719 [==============================] - 2s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.5247e+00 - 746ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.3250e+00 - 454ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.6710e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -2.8719e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -2.9971e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -3.0953e+00 - 451ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -3.1063e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -3.2198e+00 - 453ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -3.2509e+00 - 452ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -3.2943e+00 - 452ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 758us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.5843e-01 - 740ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -4.9138e-01 - 443ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -7.6205e-01 - 444ms/epoch - 1ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -8.9299e-01 - 444ms/epoch - 1ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -9.8665e-01 - 444ms/epoch - 1ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -1.0673e+00 - 445ms/epoch - 1ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -1.1409e+00 - 444ms/epoch - 1ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -1.2187e+00 - 442ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -1.2323e+00 - 443ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -1.2914e+00 - 443ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 756us/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.2406e-01 - 719ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -2.6177e-01 - 428ms/epoch - 995us/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.9399e-01 - 428ms/epoch - 994us/step\n",
      "Epoch 4/10\n",
      "430/430 - 0s - loss: -3.3251e-01 - 428ms/epoch - 995us/step\n",
      "Epoch 5/10\n",
      "430/430 - 0s - loss: -4.0927e-01 - 427ms/epoch - 992us/step\n",
      "Epoch 6/10\n",
      "430/430 - 0s - loss: -4.8515e-01 - 428ms/epoch - 995us/step\n",
      "Epoch 7/10\n",
      "430/430 - 0s - loss: -5.3363e-01 - 429ms/epoch - 997us/step\n",
      "Epoch 8/10\n",
      "430/430 - 0s - loss: -5.8245e-01 - 436ms/epoch - 1ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 0s - loss: -6.1481e-01 - 436ms/epoch - 1ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 0s - loss: -6.5753e-01 - 434ms/epoch - 1ms/step\n",
      "1719/1719 [==============================] - 1s 731us/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -2.7406e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -4.0816e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -4.8464e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -4.9406e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -4.8357e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -5.3444e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -5.8036e+00 - 2s/epoch - 6ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -6.1679e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -6.0948e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -7.3545e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1049e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -2.0790e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -2.5147e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -2.7425e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -2.9243e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -3.0963e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -3.2409e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -3.2926e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -3.4022e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -3.5113e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 3s 2ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 2s - loss: -1.1153e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 2s - loss: -3.6182e-01 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 2s - loss: -6.0480e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "430/430 - 2s - loss: -8.0147e-01 - 2s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "430/430 - 2s - loss: -9.4020e-01 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "430/430 - 2s - loss: -1.1058e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "430/430 - 2s - loss: -1.1978e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "430/430 - 2s - loss: -1.2541e+00 - 2s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "430/430 - 2s - loss: -1.3391e+00 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "430/430 - 2s - loss: -1.4174e+00 - 2s/epoch - 4ms/step\n",
      "1719/1719 [==============================] - 2s 1ms/step\n",
      "Epoch 1/10\n",
      "430/430 - 1s - loss: -1.0382e+00 - 737ms/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "430/430 - 0s - loss: -1.8100e+00 - 443ms/epoch - 1ms/step\n",
      "Epoch 3/10\n",
      "430/430 - 0s - loss: -2.1650e+00 - 439ms/epoch - 1ms/step\n",
      "Epoch 4/10\n"
     ]
    }
   ],
   "source": [
    "# Append the data lists to the dictionary for each iteration\n",
    "data_dict = {\"accuracies\": [],\n",
    "             \"losses\": [],\n",
    "             \"MI_estimate_x_d1\": [],\n",
    "             \"MI_estimate_x_d2\": [],\n",
    "             \"MI_estimate_x_o\": [],\n",
    "             \"MI_estimate_d1_d2\": [],\n",
    "             \"MI_estimate_d1_o\": [],\n",
    "             \"MI_estimate_d2_o\": [],\n",
    "             \"MI_hist_x_d1\": [],\n",
    "             \"MI_hist_x_d2\": [],\n",
    "             \"MI_hist_x_o\": [],\n",
    "             \"MI_hist_d1_d2\": [],\n",
    "             \"MI_hist_d1_o\": [],\n",
    "             \"MI_hist_d2_o\": [],\n",
    "             \"accuracies_init\": [],\n",
    "             \"losses_init\": [],\n",
    "             \"MI_estimate_x_d1_init\": [],\n",
    "             \"MI_estimate_x_d2_init\": [],\n",
    "             \"MI_estimate_x_o_init\": [],\n",
    "             \"MI_estimate_d1_d2_init\": [],\n",
    "             \"MI_estimate_d1_o_init\": [],\n",
    "             \"MI_estimate_d2_o_init\": [],\n",
    "             \"MI_hist_x_d1_init\": [],\n",
    "             \"MI_hist_x_d2_init\": [],\n",
    "             \"MI_hist_x_o_init\": [],\n",
    "             \"MI_hist_d1_d2_init\": [],\n",
    "             \"MI_hist_d1_o_init\": [],\n",
    "             \"MI_hist_d2_o_init\": [],\n",
    "             \"accuracies_rand\": [],\n",
    "             \"losses_rand\": [],\n",
    "             \"MI_estimate_x_d1_rand\": [],\n",
    "             \"MI_estimate_x_d2_rand\": [],\n",
    "             \"MI_estimate_x_o_rand\": [],\n",
    "             \"MI_estimate_d1_d2_rand\": [],\n",
    "             \"MI_estimate_d1_o_rand\": [],\n",
    "             \"MI_estimate_d2_o_rand\": [],\n",
    "             \"MI_hist_x_d1_rand\": [],\n",
    "             \"MI_hist_x_d2_rand\": [],\n",
    "             \"MI_hist_x_o_rand\": [],\n",
    "             \"MI_hist_d1_d2_rand\": [],\n",
    "             \"MI_hist_d1_o_rand\": [],\n",
    "             \"MI_hist_d2_o_rand\": [],\n",
    "             }\n",
    "\n",
    "# We do 10 runs to get an average\n",
    "\n",
    "for j in range(averaging_iterations):\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"Experimental run number: \" + str(j+1))\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    init_model = keras.models.clone_model(base_model)\n",
    "    pruner = LotteryTicketPruner(init_model) # pruner set-up\n",
    "    pruner2 = LotteryTicketPruner(init_model)\n",
    "    \n",
    "    # collected outputs for evaluation\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    MI_estimate_x_d1 = []\n",
    "    MI_estimate_x_d2 = []\n",
    "    MI_estimate_x_o = []\n",
    "    MI_estimate_d1_d2 = []\n",
    "    MI_estimate_d1_o = []\n",
    "    MI_estimate_d2_o = []\n",
    "    MI_hist_x_d1 = []\n",
    "    MI_hist_x_d2 = []\n",
    "    MI_hist_x_o = []\n",
    "    MI_hist_d1_d2 = []\n",
    "    MI_hist_d1_o = []\n",
    "    MI_hist_d2_o = []\n",
    "    \n",
    "    accuracies_init = []\n",
    "    losses_init = []\n",
    "    MI_estimate_x_d1_init = []\n",
    "    MI_estimate_x_d2_init = []\n",
    "    MI_estimate_x_o_init = []\n",
    "    MI_estimate_d1_d2_init = []\n",
    "    MI_estimate_d1_o_init = []\n",
    "    MI_estimate_d2_o_init = []\n",
    "    MI_hist_x_d1_init = []\n",
    "    MI_hist_x_d2_init = []\n",
    "    MI_hist_x_o_init = []\n",
    "    MI_hist_d1_d2_init = []\n",
    "    MI_hist_d1_o_init = []\n",
    "    MI_hist_d2_o_init = []\n",
    "    \n",
    "    accuracies_rand = []\n",
    "    losses_rand = []\n",
    "    MI_estimate_x_d1_rand = []\n",
    "    MI_estimate_x_d2_rand = []\n",
    "    MI_estimate_x_o_rand = []\n",
    "    MI_estimate_d1_d2_rand = []\n",
    "    MI_estimate_d1_o_rand = []\n",
    "    MI_estimate_d2_o_rand = []\n",
    "    MI_hist_x_d1_rand = []\n",
    "    MI_hist_x_d2_rand = []\n",
    "    MI_hist_x_o_rand = []\n",
    "    MI_hist_d1_d2_rand = []\n",
    "    MI_hist_d1_o_rand = []\n",
    "    MI_hist_d2_o_rand = []\n",
    "\n",
    "    # compiling model with training params\n",
    "    model = keras.models.clone_model(base_model)\n",
    "    model2 = keras.models.clone_model(base_model)\n",
    "    model.load_weights(\"init_weights.h5\")\n",
    "    model2.load_weights(\"init_weights.h5\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "                  # Loss function to minimize\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "                  # List of metrics to monitor\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                 )\n",
    "    model2.compile(optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "                  # Loss function to minimize\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "                  # List of metrics to monitor\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                 )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    pre_train_loss, pre_train_accuracy = model.evaluate(x_test, y_test)\n",
    "    accuracies.append(pre_train_accuracy)\n",
    "    accuracies_init.append(pre_train_accuracy)\n",
    "    accuracies_rand.append(pre_train_accuracy) # we'll save time by appending to rand outputs because same before prune\n",
    "    print(\"acc appended\")\n",
    "    losses.append(pre_train_loss)\n",
    "    losses_init.append(pre_train_loss)\n",
    "    losses_rand.append(pre_train_loss) # same for losses, before pruning there's no structural difference yet\n",
    "    print(\"loss appended\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"attempting mutual information neural estimation\")\n",
    "    fit_loss_history_x_d1, mutual_info_x_d1 = get_mine_x_d1(model)\n",
    "    fit_loss_history_x_d2, mutual_info_x_d2 = get_mine_x_d2(model)\n",
    "    fit_loss_history_x_o, mutual_info_x_o = get_mine_x_o(model)\n",
    "    fit_loss_history_d1_d2, mutual_info_d1_d2 = get_mine_d1_d2(model)\n",
    "    fit_loss_history_d1_o, mutual_info_d1_o = get_mine_d1_o(model)\n",
    "    fit_loss_history_d2_o, mutual_info_d2_o = get_mine_d2_o(model)\n",
    "    print(\"estimated mutual information x_d1: \" + str(mutual_info_x_d1))\n",
    "    print(\"estimated mutual information x_d2: \" + str(mutual_info_x_d2))\n",
    "    print(\"estimated mutual information x_o: \" + str(mutual_info_x_o))\n",
    "    print(\"estimated mutual information d1_d2: \" + str(mutual_info_d1_d2))\n",
    "    print(\"estimated mutual information d1_o: \" + str(mutual_info_d1_o))\n",
    "    print(\"estimated mutual information d2_o: \" + str(mutual_info_d2_o))\n",
    "    MI_estimate_x_d1.append(mutual_info_x_d1)\n",
    "    MI_estimate_x_d2.append(mutual_info_x_d2)\n",
    "    MI_estimate_x_o.append(mutual_info_x_o)\n",
    "    MI_estimate_d1_d2.append(mutual_info_d1_d2)\n",
    "    MI_estimate_d1_o.append(mutual_info_d1_o)\n",
    "    MI_estimate_d2_o.append(mutual_info_d2_o)\n",
    "    MI_estimate_x_d1_init.append(mutual_info_x_d1) # those are already init_weights\n",
    "    MI_estimate_x_d2_init.append(mutual_info_x_d2)\n",
    "    MI_estimate_x_o_init.append(mutual_info_x_o)\n",
    "    MI_estimate_d1_d2_init.append(mutual_info_d1_d2)\n",
    "    MI_estimate_d1_o_init.append(mutual_info_d1_o)\n",
    "    MI_estimate_d2_o_init.append(mutual_info_d2_o)\n",
    "    MI_estimate_x_d1_rand.append(mutual_info_x_d1) # append same as model because no pruning has been done yet\n",
    "    MI_estimate_x_d2_rand.append(mutual_info_x_d2)\n",
    "    MI_estimate_x_o_rand.append(mutual_info_x_o)\n",
    "    MI_estimate_d1_d2_rand.append(mutual_info_d1_d2)\n",
    "    MI_estimate_d1_o_rand.append(mutual_info_d1_o)\n",
    "    MI_estimate_d2_o_rand.append(mutual_info_d2_o)\n",
    "    print(\"mutual info appended\")\n",
    "    MI_hist_x_d1.append(fit_loss_history_x_d1)\n",
    "    MI_hist_x_d2.append(fit_loss_history_x_d2)\n",
    "    MI_hist_x_o.append(fit_loss_history_x_o)\n",
    "    MI_hist_d1_d2.append(fit_loss_history_d1_d2)\n",
    "    MI_hist_d1_o.append(fit_loss_history_d1_o)\n",
    "    MI_hist_d2_o.append(fit_loss_history_d2_o)\n",
    "    MI_hist_x_d1_init.append(fit_loss_history_x_d1) # same for losses, it's init_weights\n",
    "    MI_hist_x_d2_init.append(fit_loss_history_x_d2)\n",
    "    MI_hist_x_o_init.append(fit_loss_history_x_o)\n",
    "    MI_hist_d1_d2_init.append(fit_loss_history_d1_d2)\n",
    "    MI_hist_d1_o_init.append(fit_loss_history_d1_o)\n",
    "    MI_hist_d2_o_init.append(fit_loss_history_d2_o)\n",
    "    MI_hist_x_d1_rand.append(fit_loss_history_x_d1)\n",
    "    MI_hist_x_d2_rand.append(fit_loss_history_x_d2)\n",
    "    MI_hist_x_o_rand.append(fit_loss_history_x_o)\n",
    "    MI_hist_d1_d2_rand.append(fit_loss_history_d1_d2)\n",
    "    MI_hist_d1_o_rand.append(fit_loss_history_d1_o)\n",
    "    MI_hist_d2_o_rand.append(fit_loss_history_d2_o)\n",
    "    print(\"fit_loss_history appended\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"fully connected model, pre-train: \" + \"loss: \" + str(pre_train_loss) + \" acc: \" + str(pre_train_accuracy))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"sanity check\")\n",
    "    print(\"output layer weight mask, pre-train, first 10 arr: \")\n",
    "    print(model.layers[3].get_weights()[0][:10])\n",
    "\n",
    "    # fully-connected trained\n",
    "    model.load_weights(\"trained_weights.h5\") # LOADING pre-trained weights for reproducibility\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "                  # Loss function to minimize\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "                  # List of metrics to monitor\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                 )\n",
    "    model2.load_weights(\"trained_weights.h5\") # LOADING pre-trained weights for reproducibility\n",
    "    model2.compile(optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "                  # Loss function to minimize\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "                  # List of metrics to monitor\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                  )\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    trained_loss, trained_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(\"fully connected model, trained: \" + \"loss: \" + str(trained_loss) + \" acc: \" + str(trained_accuracy))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    accuracies.append(trained_accuracy) \n",
    "    accuracies_init.append(pre_train_accuracy) # if we take trained model and put init_weights, we get the init_model\n",
    "    accuracies_rand.append(trained_accuracy) # no prune yet, so same\n",
    "    print(\"acc appended\")\n",
    "    losses.append(trained_loss)\n",
    "    losses_init.append(pre_train_loss) # s.a.\n",
    "    losses_rand.append(trained_loss) # s.a.\n",
    "    print(\"loss appended\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    MI_estimate_x_d1_init.append(mutual_info_x_d1) # appending early, because I reuse variable names\n",
    "    MI_estimate_x_d2_init.append(mutual_info_x_d2) # the logic is if we have the fully trained model and reset\n",
    "    MI_estimate_x_o_init.append(mutual_info_x_o)   # the weights to init_weights, we just get the init_model from above again\n",
    "    MI_estimate_d1_d2_init.append(mutual_info_d1_d2) \n",
    "    MI_estimate_d1_o_init.append(mutual_info_d1_o)\n",
    "    MI_estimate_d2_o_init.append(mutual_info_d2_o)\n",
    "    MI_hist_x_d1_init.append(fit_loss_history_x_d1)\n",
    "    MI_hist_x_d2_init.append(fit_loss_history_x_d2)\n",
    "    MI_hist_x_o_init.append(fit_loss_history_x_o)\n",
    "    MI_hist_d1_d2_init.append(fit_loss_history_d1_d2)\n",
    "    MI_hist_d1_o_init.append(fit_loss_history_d1_o)\n",
    "    MI_hist_d2_o_init.append(fit_loss_history_d2_o)\n",
    "    print(\"early append of _init done\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"attempting mutual information neural estimation\")\n",
    "    fit_loss_history_x_d1, mutual_info_x_d1 = get_mine_x_d1(model)\n",
    "    fit_loss_history_x_d2, mutual_info_x_d2 = get_mine_x_d2(model)\n",
    "    fit_loss_history_x_o, mutual_info_x_o = get_mine_x_o(model)\n",
    "    fit_loss_history_d1_d2, mutual_info_d1_d2 = get_mine_d1_d2(model)\n",
    "    fit_loss_history_d1_o, mutual_info_d1_o = get_mine_d1_o(model)\n",
    "    fit_loss_history_d2_o, mutual_info_d2_o = get_mine_d2_o(model)\n",
    "    print(\"estimated mutual information x_d1: \" + str(mutual_info_x_d1))\n",
    "    print(\"estimated mutual information x_d2: \" + str(mutual_info_x_d2))\n",
    "    print(\"estimated mutual information x_o: \" + str(mutual_info_x_o))\n",
    "    print(\"estimated mutual information d1_d2: \" + str(mutual_info_d1_d2))\n",
    "    print(\"estimated mutual information d1_o: \" + str(mutual_info_d1_o))\n",
    "    print(\"estimated mutual information d2_o: \" + str(mutual_info_d2_o))\n",
    "    MI_estimate_x_d1.append(mutual_info_x_d1)\n",
    "    MI_estimate_x_d2.append(mutual_info_x_d2)\n",
    "    MI_estimate_x_o.append(mutual_info_x_o)\n",
    "    MI_estimate_d1_d2.append(mutual_info_d1_d2) # same as above, same model pre-pruning\n",
    "    MI_estimate_d1_o.append(mutual_info_d1_o)\n",
    "    MI_estimate_d2_o.append(mutual_info_d2_o)\n",
    "    MI_estimate_x_d1_rand.append(mutual_info_x_d1)\n",
    "    MI_estimate_x_d2_rand.append(mutual_info_x_d2)\n",
    "    MI_estimate_x_o_rand.append(mutual_info_x_o)\n",
    "    MI_estimate_d1_d2_rand.append(mutual_info_d1_d2) \n",
    "    MI_estimate_d1_o_rand.append(mutual_info_d1_o)\n",
    "    MI_estimate_d2_o_rand.append(mutual_info_d2_o)\n",
    "    print(\"mutual info appended\")\n",
    "    MI_hist_x_d1.append(fit_loss_history_x_d1)\n",
    "    MI_hist_x_d2.append(fit_loss_history_x_d2)\n",
    "    MI_hist_x_o.append(fit_loss_history_x_o)\n",
    "    MI_hist_d1_d2.append(fit_loss_history_d1_d2)\n",
    "    MI_hist_d1_o.append(fit_loss_history_d1_o)\n",
    "    MI_hist_d2_o.append(fit_loss_history_d2_o)\n",
    "    MI_hist_x_d1_rand.append(fit_loss_history_x_d1) # model pre-pruning, same weights, same histories\n",
    "    MI_hist_x_d2_rand.append(fit_loss_history_x_d2)\n",
    "    MI_hist_x_o_rand.append(fit_loss_history_x_o)\n",
    "    MI_hist_d1_d2_rand.append(fit_loss_history_d1_d2)\n",
    "    MI_hist_d1_o_rand.append(fit_loss_history_d1_o)\n",
    "    MI_hist_d2_o_rand.append(fit_loss_history_d2_o)\n",
    "    print(\"fit_loss_history appended\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"sanity check\")\n",
    "    print(\"output layer weight mask, trained, fully connected model, first 10 arr: \")\n",
    "    print(model.layers[3].get_weights()[0][:10] != 0)\n",
    "\n",
    "    for i in range(pruning_iterations):\n",
    "        pruner.set_pretrained_weights(model) # pruner for large_final pruning schedule\n",
    "        pruner2.set_pretrained_weights(model2) # pruner for random pruning schedule\n",
    "        model.set_weights(init_weights)\n",
    "        model2.set_weights(init_weights)\n",
    "        pruner.calc_prune_mask(model, pruning_rate,'large_final')\n",
    "        pruner2.calc_prune_mask(model2, pruning_rate, 'random')\n",
    "        print(\"prune_mask calculated\")\n",
    "        print(\"\")\n",
    "        sparsity = calc_sparsity(i,pruning_rate)\n",
    "        print(f\"Iteration {i+1}: making {sparsity:.2f}% sparse large_final\")\n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=0,\n",
    "                            # monitoring validation loss and metrics\n",
    "                            # at the end of each epoch\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            callbacks=[PrunerCallback(pruner)])\n",
    "                   \n",
    "        print(f\"Iteration {i+1}: making {sparsity:.2f}% sparse random\")\n",
    "        history = model2.fit(x_train,\n",
    "                             y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             verbose=0,\n",
    "                             # monitoring validation loss and metrics\n",
    "                             # at the end of each epoch\n",
    "                             validation_data=(x_val, y_val),\n",
    "                             callbacks=[PrunerCallback(pruner2)])\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        ticket_loss, ticket_accuracy = model.evaluate(x_test, y_test)\n",
    "        print(f\"{sparsity:.2f}% sparse large_final: \" + \"loss: \" + str(ticket_loss) + \" acc: \" + str(ticket_accuracy))\n",
    "        model_init = keras.models.clone_model(model) # cloning the ticket so my set_function doesn't interfere with the iteration\n",
    "        model_init = set_model(init_model, model_init)\n",
    "        model_init.compile(optimizer=keras.optimizers.Adam(learning_rate=1.2e-3), # Adam optimizer, lr=0.0012\n",
    "                  # Loss function to minimize\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(), # multi-class classification loss function\n",
    "                  # List of metrics to monitor\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                  )# using my set_function to take the ticket and manually set non-zero weights to init_weights\n",
    "        ticket_loss_init, ticket_accuracy_init = model_init.evaluate(x_test, y_test)\n",
    "        print(f\"{sparsity:.2f}% sparse large_final, init_weights: \" + \"loss: \" + str(ticket_loss_init) + \" acc: \" + str(ticket_accuracy_init))\n",
    "        random_loss, random_accuracy = model2.evaluate(x_test, y_test)\n",
    "        print(f\"{sparsity:.2f}% sparse random: \" + \"loss: \" + str(random_loss) + \" acc: \" + str(random_accuracy))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        accuracies.append(ticket_accuracy)\n",
    "        accuracies_init.append(ticket_accuracy_init)\n",
    "        accuracies_rand.append(random_accuracy)\n",
    "        print(\"acc appended\")\n",
    "        losses.append(ticket_loss)\n",
    "        losses_init.append(ticket_loss_init)\n",
    "        losses_rand.append(random_loss)\n",
    "        print(\"loss appended\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"attempting mutual information neural estimation\")\n",
    "        fit_loss_history_x_d1, mutual_info_x_d1 = get_mine_x_d1(model)\n",
    "        fit_loss_history_x_d2, mutual_info_x_d2 = get_mine_x_d2(model)\n",
    "        fit_loss_history_x_o, mutual_info_x_o = get_mine_x_o(model)\n",
    "        fit_loss_history_d1_d2, mutual_info_d1_d2 = get_mine_d1_d2(model)\n",
    "        fit_loss_history_d1_o, mutual_info_d1_o = get_mine_d1_o(model)\n",
    "        fit_loss_history_d2_o, mutual_info_d2_o = get_mine_d2_o(model)\n",
    "        fit_loss_history_x_d1_init, mutual_info_x_d1_init = get_mine_x_d1(model_init)\n",
    "        fit_loss_history_x_d2_init, mutual_info_x_d2_init = get_mine_x_d2(model_init)\n",
    "        fit_loss_history_x_o_init, mutual_info_x_o_init = get_mine_x_o(model_init)\n",
    "        fit_loss_history_d1_d2_init, mutual_info_d1_d2_init = get_mine_d1_d2(model_init)\n",
    "        fit_loss_history_d1_o_init, mutual_info_d1_o_init = get_mine_d1_o(model_init)\n",
    "        fit_loss_history_d2_o_init, mutual_info_d2_o_init = get_mine_d2_o(model_init)\n",
    "        fit_loss_history_x_d1_rand, mutual_info_x_d1_rand = get_mine_x_d1(model2)\n",
    "        fit_loss_history_x_d2_rand, mutual_info_x_d2_rand = get_mine_x_d2(model2)\n",
    "        fit_loss_history_x_o_rand, mutual_info_x_o_rand = get_mine_x_o(model2)\n",
    "        fit_loss_history_d1_d2_rand, mutual_info_d1_d2_rand = get_mine_d1_d2(model2)\n",
    "        fit_loss_history_d1_o_rand, mutual_info_d1_o_rand = get_mine_d1_o(model2)\n",
    "        fit_loss_history_d2_o_rand, mutual_info_d2_o_rand = get_mine_d2_o(model2)\n",
    "        print(\"estimated mutual information x_d1, large_final: \" + str(mutual_info_x_d1))\n",
    "        print(\"estimated mutual information x_d2, large_final: \" + str(mutual_info_x_d2))\n",
    "        print(\"estimated mutual information x_o, large_final: \" + str(mutual_info_x_o))\n",
    "        print(\"estimated mutual information d1_d2, large_final: \" + str(mutual_info_d1_d2))\n",
    "        print(\"estimated mutual information d1_o, large_final: \" + str(mutual_info_d1_o))\n",
    "        print(\"estimated mutual information d2_o, large_final: \" + str(mutual_info_d2_o))\n",
    "        print(\"estimated mutual information x_d1, large_final, init_weights: \" + str(mutual_info_x_d1_init))\n",
    "        print(\"estimated mutual information x_d2, large_final, init_weights: \" + str(mutual_info_x_d2_init))\n",
    "        print(\"estimated mutual information x_o, large_final, init_weights: \" + str(mutual_info_x_o_init))\n",
    "        print(\"estimated mutual information d1_d2, large_final, init_weights: \" + str(mutual_info_d1_d2_init))\n",
    "        print(\"estimated mutual information d1_o, large_final, init_weights: \" + str(mutual_info_d1_o_init))\n",
    "        print(\"estimated mutual information d2_o, large_final, init_weights: \" + str(mutual_info_d2_o_init))\n",
    "        print(\"estimated mutual information x_d1, random: \" + str(mutual_info_x_d1_rand))\n",
    "        print(\"estimated mutual information x_d2, random: \" + str(mutual_info_x_d2_rand))\n",
    "        print(\"estimated mutual information x_o, random: \" + str(mutual_info_x_o_rand))\n",
    "        print(\"estimated mutual information d1_d2, random: \" + str(mutual_info_d1_d2_rand))\n",
    "        print(\"estimated mutual information d1_o, random: \" + str(mutual_info_d1_o_rand))\n",
    "        print(\"estimated mutual information d2_o, random: \" + str(mutual_info_d2_o_rand))\n",
    "        MI_estimate_x_d1.append(mutual_info_x_d1)\n",
    "        MI_estimate_x_d2.append(mutual_info_x_d2)\n",
    "        MI_estimate_x_o.append(mutual_info_x_o)\n",
    "        MI_estimate_d1_d2.append(mutual_info_d1_d2)\n",
    "        MI_estimate_d1_o.append(mutual_info_d1_o)\n",
    "        MI_estimate_d2_o.append(mutual_info_d2_o)\n",
    "        MI_estimate_x_d1_init.append(mutual_info_x_d1_init)\n",
    "        MI_estimate_x_d2_init.append(mutual_info_x_d2_init)\n",
    "        MI_estimate_x_o_init.append(mutual_info_x_o_init)\n",
    "        MI_estimate_d1_d2_init.append(mutual_info_d1_d2_init)\n",
    "        MI_estimate_d1_o_init.append(mutual_info_d1_o_init)\n",
    "        MI_estimate_d2_o_init.append(mutual_info_d2_o_init)\n",
    "        MI_estimate_x_d1_rand.append(mutual_info_x_d1_rand)\n",
    "        MI_estimate_x_d2_rand.append(mutual_info_x_d2_rand)\n",
    "        MI_estimate_x_o_rand.append(mutual_info_x_o_rand)\n",
    "        MI_estimate_d1_d2_rand.append(mutual_info_d1_d2_rand)\n",
    "        MI_estimate_d1_o_rand.append(mutual_info_d1_o_rand)\n",
    "        MI_estimate_d2_o_rand.append(mutual_info_d2_o_rand)\n",
    "        print(\"mutual info appended\")\n",
    "        MI_hist_x_d1.append(fit_loss_history_x_d1)\n",
    "        MI_hist_x_d2.append(fit_loss_history_x_d2)\n",
    "        MI_hist_x_o.append(fit_loss_history_x_o)\n",
    "        MI_hist_d1_d2.append(fit_loss_history_d1_d2)\n",
    "        MI_hist_d1_o.append(fit_loss_history_d1_o)\n",
    "        MI_hist_d2_o.append(fit_loss_history_d2_o)\n",
    "        MI_hist_x_d1_init.append(fit_loss_history_x_d1_init)\n",
    "        MI_hist_x_d2_init.append(fit_loss_history_x_d2_init)\n",
    "        MI_hist_x_o_init.append(fit_loss_history_x_o_init)\n",
    "        MI_hist_d1_d2_init.append(fit_loss_history_d1_d2_init)\n",
    "        MI_hist_d1_o_init.append(fit_loss_history_d1_o_init)\n",
    "        MI_hist_d2_o_init.append(fit_loss_history_d2_o_init)\n",
    "        MI_hist_x_d1_rand.append(fit_loss_history_x_d1_rand)\n",
    "        MI_hist_x_d2_rand.append(fit_loss_history_x_d2_rand)\n",
    "        MI_hist_x_o_rand.append(fit_loss_history_x_o_rand)\n",
    "        MI_hist_d1_d2_rand.append(fit_loss_history_d1_d2_rand)\n",
    "        MI_hist_d1_o_rand.append(fit_loss_history_d1_o_rand)\n",
    "        MI_hist_d2_o_rand.append(fit_loss_history_d2_o_rand)\n",
    "        print(\"fit_loss_history appended\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"sanity check\")\n",
    "        print(f\"output layer weight mask, {sparsity:.2f}% sparse large_final, first 10 arr: \")\n",
    "        print(model.layers[3].get_weights()[0][:10])\n",
    "        print(f\"output layer weight mask, {sparsity:.2f}% sparse large_final, init_weights, first 10 arr: \")\n",
    "        print(model_init.layers[3].get_weights()[0][:10])\n",
    "        print(f\"output layer weight mask, {sparsity:.2f}% sparse random, first 10 arr: \")\n",
    "        print(model2.layers[3].get_weights()[0][:10])\n",
    "        \n",
    "    # saving data for averaging     \n",
    "    data_dict[\"accuracies\"].append(accuracies)\n",
    "    data_dict[\"losses\"].append(losses)\n",
    "    data_dict[\"MI_estimate_x_d1\"].append(MI_estimate_x_d1)\n",
    "    data_dict[\"MI_estimate_x_d2\"].append(MI_estimate_x_d2)\n",
    "    data_dict[\"MI_estimate_x_o\"].append(MI_estimate_x_o)\n",
    "    data_dict[\"MI_estimate_d1_d2\"].append(MI_estimate_d1_d2)\n",
    "    data_dict[\"MI_estimate_d1_o\"].append(MI_estimate_d1_o)\n",
    "    data_dict[\"MI_estimate_d2_o\"].append(MI_estimate_d2_o)\n",
    "    data_dict[\"MI_hist_x_d1\"].append(MI_hist_x_d1)\n",
    "    data_dict[\"MI_hist_x_d2\"].append(MI_hist_x_d2)\n",
    "    data_dict[\"MI_hist_x_o\"].append(MI_hist_x_o)\n",
    "    data_dict[\"MI_hist_d1_d2\"].append(MI_hist_d1_d2)\n",
    "    data_dict[\"MI_hist_d1_o\"].append(MI_hist_d1_o)\n",
    "    data_dict[\"MI_hist_d2_o\"].append(MI_hist_d2_o)\n",
    "    data_dict[\"accuracies_init\"].append(accuracies_init)\n",
    "    data_dict[\"losses_init\"].append(losses_init)\n",
    "    data_dict[\"MI_estimate_x_d1_init\"].append(MI_estimate_x_d1_init)\n",
    "    data_dict[\"MI_estimate_x_d2_init\"].append(MI_estimate_x_d2_init)\n",
    "    data_dict[\"MI_estimate_x_o_init\"].append(MI_estimate_x_o_init)\n",
    "    data_dict[\"MI_estimate_d1_d2_init\"].append(MI_estimate_d1_d2_init)\n",
    "    data_dict[\"MI_estimate_d1_o_init\"].append(MI_estimate_d1_o_init)\n",
    "    data_dict[\"MI_estimate_d2_o_init\"].append(MI_estimate_d2_o_init)\n",
    "    data_dict[\"MI_hist_x_d1_init\"].append(MI_hist_x_d1_init)\n",
    "    data_dict[\"MI_hist_x_d2_init\"].append(MI_hist_x_d2_init)\n",
    "    data_dict[\"MI_hist_x_o_init\"].append(MI_hist_x_o_init)\n",
    "    data_dict[\"MI_hist_d1_d2_init\"].append(MI_hist_d1_d2_init)\n",
    "    data_dict[\"MI_hist_d1_o_init\"].append(MI_hist_d1_o_init)\n",
    "    data_dict[\"MI_hist_d2_o_init\"].append(MI_hist_d2_o_init)\n",
    "    data_dict[\"accuracies_rand\"].append(accuracies_rand)\n",
    "    data_dict[\"losses_rand\"].append(losses_rand)\n",
    "    data_dict[\"MI_estimate_x_d1_rand\"].append(MI_estimate_x_d1_rand)\n",
    "    data_dict[\"MI_estimate_x_d2_rand\"].append(MI_estimate_x_d2_rand)\n",
    "    data_dict[\"MI_estimate_x_o_rand\"].append(MI_estimate_x_o_rand)\n",
    "    data_dict[\"MI_estimate_d1_d2_rand\"].append(MI_estimate_d1_d2_rand)\n",
    "    data_dict[\"MI_estimate_d1_o_rand\"].append(MI_estimate_d1_o_rand)\n",
    "    data_dict[\"MI_estimate_d2_o_rand\"].append(MI_estimate_d2_o_rand)\n",
    "    data_dict[\"MI_hist_x_d1_rand\"].append(MI_hist_x_d1_rand)\n",
    "    data_dict[\"MI_hist_x_d2_rand\"].append(MI_hist_x_d2_rand)\n",
    "    data_dict[\"MI_hist_x_o_rand\"].append(MI_hist_x_o_rand)\n",
    "    data_dict[\"MI_hist_d1_d2_rand\"].append(MI_hist_d1_d2_rand)\n",
    "    data_dict[\"MI_hist_d1_o_rand\"].append(MI_hist_d1_o_rand)\n",
    "    data_dict[\"MI_hist_d2_o_rand\"].append(MI_hist_d2_o_rand)\n",
    "    print(f\"Experiment {j+1} saved in Dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3572646",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ticks = [i for i in range(pruning_iterations+2)] # custom ticks for averaged graphs\n",
    "custom_labels = ['NT', 'FT'] + [f\"{round(calc_sparsity(i, pruning_rate), 2)}%\" for i in range(pruning_iterations)] # see above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_ticks)\n",
    "print(custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfc64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(np.average(data_dict[\"accuracies\"], axis=0), 'o-', label=\"large_final\")\n",
    "plt.plot(np.average(data_dict[\"accuracies_init\"], axis=0), 'o-', label=\"large_final, init\")\n",
    "plt.plot(np.average(data_dict[\"accuracies_rand\"], axis=0), 'o-', label=\"random\")\n",
    "\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(f'Accuracy as models get more sparse, average of {averaging_iterations} runs')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(np.average(data_dict[\"losses\"], axis=0), 'o-', label=\"large_final\")\n",
    "plt.plot(np.average(data_dict[\"losses_init\"], axis=0), 'o-', label=\"large_final, init\")\n",
    "plt.plot(np.average(data_dict[\"losses_rand\"], axis=0), 'o-', label=\"random\")\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('loss')\n",
    "plt.title(f'Loss as models get more sparse, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832393d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d1\"], axis=0), 'o-', label=\"x_d1_LF\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d2\"], axis=0), 'o-', label=\"x_d2_LF\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_o\"], axis=0), 'o-', label=\"x_o_LF\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_d2\"], axis=0), 'o-', label=\"d1_d2_LF\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_o\"], axis=0), 'o-', label=\"d1_o_LF\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d2_o\"], axis=0), 'o-', label=\"d2_o_LF\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d1_init\"], axis=0), 'o-', label=\"x_d1_LF_init\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d2_init\"], axis=0), 'o-', label=\"x_d2_LF_init\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_o_init\"], axis=0), 'o-', label=\"x_o_LF_init\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_d2_init\"], axis=0), 'o-', label=\"d1_d2_LF_init\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_o_init\"], axis=0), 'o-', label=\"d1_o_LF_init\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d2_o_init\"], axis=0), 'o-', label=\"d2_o_LF_init\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d1_rand\"], axis=0), 'o-', label=\"x_d1_random\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d2_rand\"], axis=0), 'o-', label=\"x_d2_random\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_o_rand\"], axis=0), 'o-', label=\"x_o_random\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_d2_rand\"], axis=0), 'o-', label=\"d1_d2_random\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_o_rand\"], axis=0), 'o-', label=\"d1_o_random\")\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d2_o_rand\"], axis=0), 'o-', label=\"d2_o_random\")\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimates of layers in models of varying sparsity, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef125da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d1\"], axis=0), \"o-\", color=\"b\", linewidth=4, label=\"x_d1_LF\")\n",
    "for graph in data_dict[\"MI_estimate_x_d1\"]:\n",
    "    plt.plot(graph, \"o\", color=\"b\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d1_init\"], axis=0), \"o-\", color=\"g\", linewidth=4, label=\"x_d1_LF_init\")\n",
    "for graph in data_dict[\"MI_estimate_x_d1_init\"]:\n",
    "    plt.plot(graph, \"o\", color=\"g\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d1_rand\"], axis=0), \"o-\", color=\"r\", linewidth=4, label=\"x_d1_random\")\n",
    "for graph in data_dict[\"MI_estimate_x_d1_rand\"]:\n",
    "    plt.plot(graph, \"o\", color=\"r\", markersize=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimation of Input with first hidden layer, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d2\"], axis=0), \"o-\", color=\"b\", linewidth=4, label=\"x_d2_LF\")\n",
    "for graph in data_dict[\"MI_estimate_x_d2\"]:\n",
    "    plt.plot(graph, \"o\", color=\"b\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d2_init\"], axis=0), \"o-\", color=\"g\", linewidth=4, label=\"x_d2_LF_init\")\n",
    "for graph in data_dict[\"MI_estimate_x_d2_init\"]:\n",
    "    plt.plot(graph, \"o\", color=\"g\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_d2_rand\"], axis=0), \"o-\", color=\"r\", linewidth=4, label=\"x_d2_random\")\n",
    "for graph in data_dict[\"MI_estimate_x_d2_rand\"]:\n",
    "    plt.plot(graph, \"o\", color=\"r\", markersize=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimation of Input with second hidden layer, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_o\"], axis=0), \"o-\", color=\"b\", linewidth=4, label=\"x_o_LF\")\n",
    "for graph in data_dict[\"MI_estimate_x_o\"]:\n",
    "    plt.plot(graph, \"o\", color=\"b\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_o_init\"], axis=0), \"o-\", color=\"g\", linewidth=4, label=\"x_o_LF_init\")\n",
    "for graph in data_dict[\"MI_estimate_x_o_init\"]:\n",
    "    plt.plot(graph, \"o\", color=\"g\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_x_o_rand\"], axis=0), \"o-\", color=\"r\", linewidth=4, label=\"x_o_random\")\n",
    "for graph in data_dict[\"MI_estimate_x_o_rand\"]:\n",
    "    plt.plot(graph, \"o\", color=\"r\", markersize=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimation of Input with output layer, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_d2\"], axis=0), \"o-\", color=\"b\", linewidth=4, label=\"d1_d2_LF\")\n",
    "for graph in data_dict[\"MI_estimate_d1_d2\"]:\n",
    "    plt.plot(graph, \"o\", color=\"b\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_d2_init\"], axis=0), \"o-\", color=\"g\", linewidth=4, label=\"d1_d2_LF_init\")\n",
    "for graph in data_dict[\"MI_estimate_d1_d2_init\"]:\n",
    "    plt.plot(graph, \"o\", color=\"g\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_d2_rand\"], axis=0), \"o-\", color=\"r\", linewidth=4, label=\"d1_d2_random\")\n",
    "for graph in data_dict[\"MI_estimate_d1_d2_rand\"]:\n",
    "    plt.plot(graph, \"o\", color=\"r\", markersize=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimation of first with second hidden layer, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_o\"], axis=0), \"o-\", color=\"b\", linewidth=4, label=\"d1_o_LF\")\n",
    "for graph in data_dict[\"MI_estimate_d1_o\"]:\n",
    "    plt.plot(graph, \"o\", color=\"b\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_o_init\"], axis=0), \"o-\", color=\"g\", linewidth=4, label=\"d1_o_LF_init\")\n",
    "for graph in data_dict[\"MI_estimate_d1_o_init\"]:\n",
    "    plt.plot(graph, \"o\", color=\"g\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d1_o_rand\"], axis=0), \"o-\", color=\"r\", linewidth=4, label=\"d1_o_random\")\n",
    "for graph in data_dict[\"MI_estimate_d1_o_rand\"]:\n",
    "    plt.plot(graph, \"o\", color=\"r\", markersize=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimation of first hidden with output layer, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d2_o\"], axis=0), \"o-\", color=\"b\", linewidth=4, label=\"d2_o_LF\")\n",
    "for graph in data_dict[\"MI_estimate_d2_o\"]:\n",
    "    plt.plot(graph, \"o\", color=\"b\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d2_o_init\"], axis=0), \"o-\", color=\"g\", linewidth=4, label=\"d2_o_LF_init\")\n",
    "for graph in data_dict[\"MI_estimate_d2_o_init\"]:\n",
    "    plt.plot(graph, \"o\", color=\"g\", markersize=5, alpha=0.5)\n",
    "plt.plot(np.average(data_dict[\"MI_estimate_d2_o_rand\"], axis=0), \"o-\", color=\"r\", linewidth=4, label=\"d2_o_random\")\n",
    "for graph in data_dict[\"MI_estimate_d2_o_rand\"]:\n",
    "    plt.plot(graph, \"o\", color=\"r\", markersize=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xticks(custom_ticks,custom_labels)\n",
    "plt.xlabel('sparisty')\n",
    "plt.ylabel('MI estimate')\n",
    "plt.title(f'MI estimation of second hidden with output layer, average of {averaging_iterations} runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot each loss history with a different label\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_d1\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"-LF\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_d1_init\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"-LF_init\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_d1_rand\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"R\")\n",
    "plt.legend()\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss Histories of sparsities through training for x_d1, average of {averaging_iterations} runs')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot each loss history with a different label\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_d2\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"LF\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_d2_init\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"LF_init\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_d2_rand\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"R\")\n",
    "plt.legend()\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss Histories of sparsities through training for x_d2, average of {averaging_iterations} runs')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot each loss history with a different label\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_o\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"LF\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_o_init\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"LF_init\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_x_o_rand\"])):\n",
    "    plt.plot(-loss_history,label=custom_labels[i]+\"R\")\n",
    "plt.legend()\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss Histories of sparsities through training for x_o, average of {averaging_iterations} runs')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot each loss history with a different label\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d1_d2\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"LF\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d1_d2_init\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"LF_init\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d1_d2_rand\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"R\")\n",
    "plt.legend()\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss Histories of sparsities through training for d1_d2, average of {averaging_iterations} runs')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22704e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot each loss history with a different label\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d1_o\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"LF\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d1_o_init\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"LF_init\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d1_o_rand\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"R\")\n",
    "plt.legend()\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss Histories of sparsities through training for d1_o, average of {averaging_iterations} runs')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot each loss history with a different label\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d2_o\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"LF\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d2_o_init\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"LF_init\")\n",
    "for i, loss_history in enumerate(calculate_average_loss(data_dict[\"MI_hist_d2_o_rand\"])):\n",
    "    plt.plot(-loss_history, 'o-', label=custom_labels[i]+\"R\")\n",
    "plt.legend()\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Loss Histories of sparsities through training for d2_o, average of {averaging_iterations} runs')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
